<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0"><title>西瓜书随笔 | Ricky Li's Blog</title><meta name="author" content="Ricky Li"><meta name="copyright" content="Ricky Li"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="西瓜书随笔训练集、测试集、验证集训练集（训练）–&gt;验证集–&gt;得到结果（调参）–&gt;训练集–&gt;…. 调参到满意之后，在测试集上验证 验证集添加的目的是为了调参 存在训练集和测试集的目的是为了检测泛化能力 查准率、查全率TP：真正例 FP：假正例 FN：假反例 TN：真反例 查准率 P-percision  $P&#x3D;\frac{TP}{TP+FP}$ 查全率 R-reca">
<meta property="og:type" content="article">
<meta property="og:title" content="西瓜书随笔">
<meta property="og:url" content="http://liricky.github.io/2023/03/20/%E8%A5%BF%E7%93%9C%E4%B9%A6%E9%9A%8F%E7%AC%94/index.html">
<meta property="og:site_name" content="Ricky Li&#39;s Blog">
<meta property="og:description" content="西瓜书随笔训练集、测试集、验证集训练集（训练）–&gt;验证集–&gt;得到结果（调参）–&gt;训练集–&gt;…. 调参到满意之后，在测试集上验证 验证集添加的目的是为了调参 存在训练集和测试集的目的是为了检测泛化能力 查准率、查全率TP：真正例 FP：假正例 FN：假反例 TN：真反例 查准率 P-percision  $P&#x3D;\frac{TP}{TP+FP}$ 查全率 R-reca">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png">
<meta property="article:published_time" content="2023-03-20T06:12:42.000Z">
<meta property="article:modified_time" content="2023-03-20T06:48:26.958Z">
<meta property="article:author" content="Ricky Li">
<meta property="article:tag" content="研究生黑历史">
<meta property="article:tag" content="西瓜书">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://liricky.github.io/2023/03/20/%E8%A5%BF%E7%93%9C%E4%B9%A6%E9%9A%8F%E7%AC%94/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  }
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '西瓜书随笔',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-03-20 14:48:26'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">5</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="Ricky Li's Blog"><span class="site-name">Ricky Li's Blog</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">西瓜书随笔</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-03-20T06:12:42.000Z" title="发表于 2023-03-20 14:12:42">2023-03-20</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-03-20T06:48:26.958Z" title="更新于 2023-03-20 14:48:26">2023-03-20</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%A0%94%E7%A9%B6/">研究</a></span></div><div class="meta-secondline"></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="西瓜书随笔"><a href="#西瓜书随笔" class="headerlink" title="西瓜书随笔"></a>西瓜书随笔</h1><h2 id="训练集、测试集、验证集"><a href="#训练集、测试集、验证集" class="headerlink" title="训练集、测试集、验证集"></a>训练集、测试集、验证集</h2><p>训练集（训练）–&gt;验证集–&gt;得到结果（调参）–&gt;训练集–&gt;….</p>
<p>调参到满意之后，在测试集上验证</p>
<p><strong>验证集添加的目的是为了调参</strong></p>
<p><strong>存在训练集和测试集的目的是为了检测泛化能力</strong></p>
<h2 id="查准率、查全率"><a href="#查准率、查全率" class="headerlink" title="查准率、查全率"></a>查准率、查全率</h2><p>TP：真正例</p>
<p>FP：假正例</p>
<p>FN：假反例</p>
<p>TN：真反例</p>
<p>查准率 P-percision  $P&#x3D;\frac{TP}{TP+FP}$</p>
<p>查全率 R-recall  $R&#x3D;\frac{TP}{TP+FN}$</p>
<p><strong>数据集非均匀情况下的验证，<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV17J411C7zZ?p=13">详见</a>P13视频：使用普通准确率衡量失效（全部判断不是5也能得到较高准确率；二分类问题，是5和不是5）</strong></p>
<p>P与R反向关系图解：</p>
<p><img src="/images/%E8%A5%BF%E7%93%9C%E4%B9%A6%E9%9A%8F%E7%AC%94.assets/image-20200712204212855.png" alt="image-20200712204212855"></p>
<h2 id="F-1-度量"><a href="#F-1-度量" class="headerlink" title="$F_1$度量"></a>$F_1$度量</h2><p>$F_1$是基于查准率与查全率的调和平均定义的：$\frac{1}{F_1}&#x3D;\frac{1}{2}(\frac{1}{R}+\frac{1}{P})$；进而得出$F_1&#x3D;\frac{2<em>P</em>R}{P+R}&#x3D;\frac{2*TP}{样例总数+TP-TN}$</p>
<h2 id="x3D-x3D-F-beta-度量-x3D-x3D"><a href="#x3D-x3D-F-beta-度量-x3D-x3D" class="headerlink" title="&#x3D;&#x3D;$F_\beta$度量&#x3D;&#x3D;"></a>&#x3D;&#x3D;$F_\beta$度量&#x3D;&#x3D;</h2><p><strong>其中$\beta$&gt;0度量了查全率对查准率的相对重要性。$\beta$&#x3D;1时，退化为标准的$F_1$；$\beta$&gt;1时查全率有更大影响；$\beta$&lt;1时查准率有更大影响。</strong></p>
<p>$F_\beta$是加权调和平均：$\frac{1}{F_\beta}&#x3D;\frac{1}{1+\beta^2}(\frac{1}{P}+\frac{\beta^2}{R})&#x3D;\frac{1}{1+\beta^2}(\frac{TP+FP}{TP}+\frac{\beta^2(TP+TN)}{TP})$</p>
<p>$F_\beta&#x3D;\frac{TP(1+\beta^2)}{TP+FP+\beta^2(TP+TN)}$<img src="/images/%E8%A5%BF%E7%93%9C%E4%B9%A6%E9%9A%8F%E7%AC%94.assets/image-20200712210451846.png" alt="image-20200712210451846"></p>
<p><strong>与算数平均和几何平均相比，调和平均更重视较小值</strong></p>
<p>&#x3D;&#x3D;<strong>以上两项均为针对二分类情况</strong>&#x3D;&#x3D;</p>
<h2 id="多分类指标衡量"><a href="#多分类指标衡量" class="headerlink" title="多分类指标衡量"></a>多分类指标衡量</h2><ol>
<li>先分别计算，再求平均值</li>
<li>先平均再计算</li>
</ol>
<h2 id="ROC曲线"><a href="#ROC曲线" class="headerlink" title="ROC曲线"></a>ROC曲线</h2><p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV17J411C7zZ?p=18">P18</a>：视频中的理解画图部分有错误，红线和蓝线应该同时收束到一个点(1,1)上。</p>
<p>同趋势变化：<img src="/images/%E8%A5%BF%E7%93%9C%E4%B9%A6%E9%9A%8F%E7%AC%94.assets/image-20200712230257952.png" alt="image-20200712230257952">阈值限制的变动（竖虚线）。阈值的理解可参考下图<img src="/images/%E8%A5%BF%E7%93%9C%E4%B9%A6%E9%9A%8F%E7%AC%94.assets/image-20200712204212855-1597756822746.png" alt="image-20200712204212855"></p>
<p>$TPR&#x3D;\frac{TP}{TP+FN}$可以理解为好，$FPR&#x3D;\frac{FP}{TN+FP}$可以理解为坏。肯定希望在同样好的情况下坏的更小，或者在同样坏的情况下更好。</p>
<h2 id="AUC与rank-loss"><a href="#AUC与rank-loss" class="headerlink" title="AUC与rank-loss"></a>AUC与rank-loss</h2><p>结合视频理解吧，视频里面讲得很形象了，<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV17J411C7zZ?p=19">P19</a>、<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV17J411C7zZ?p=20">P20</a> </p>
<h2 id="x3D-x3D-代价敏感错误率与代价曲线-x3D-x3D"><a href="#x3D-x3D-代价敏感错误率与代价曲线-x3D-x3D" class="headerlink" title="&#x3D;&#x3D;代价敏感错误率与代价曲线&#x3D;&#x3D;"></a>&#x3D;&#x3D;代价敏感错误率与代价曲线&#x3D;&#x3D;</h2><ol>
<li><p>使用$p*cost_{01}$而不是$p$保证的是让关系图中为直线，而不是曲线（如果用$p$就是曲线）</p>
</li>
<li><p>分母归一化，保证使用不同$cost$描绘出的直线左右两端最高处等高（不然的话另一个$cost_{01}$代价是10，那不就右边还有横轴拉满，那还怎么比！）</p>
<p><img src="/images/%E8%A5%BF%E7%93%9C%E4%B9%A6%E9%9A%8F%E7%AC%94.assets/image-20200716172000520.png" alt="image-20200716172000520"></p>
</li>
</ol>
<h2 id="多个测试集一种算法假设检验"><a href="#多个测试集一种算法假设检验" class="headerlink" title="多个测试集一种算法假设检验"></a>多个测试集一种算法假设检验</h2><p>概率论的t分布理解：</p>
<h2 id="多元线性回归推导（来自南瓜书，详见github）"><a href="#多元线性回归推导（来自南瓜书，详见github）" class="headerlink" title="多元线性回归推导（来自南瓜书，详见github）"></a>多元线性回归推导（来自南瓜书，详见github）</h2><p>$$\cfrac{\partial E_{\hat{\boldsymbol w}}}{\partial \hat{\boldsymbol  w}}&#x3D;2\mathbf{X}^{\mathrm{T}}(\mathbf{X}\hat{\boldsymbol  w}-\boldsymbol{y})$$ [推导]：将$E_{\hat{\boldsymbol  w}}&#x3D;(\boldsymbol{y}-\mathbf{X}\hat{\boldsymbol  w})^{\mathrm{T}}(\boldsymbol{y}-\mathbf{X}\hat{\boldsymbol w})$展开可得 $$E_{\hat{\boldsymbol w}}&#x3D;  \boldsymbol{y}^{\mathrm{T}}\boldsymbol{y}-\boldsymbol{y}^{\mathrm{T}}\mathbf{X}\hat{\boldsymbol w}-\hat{\boldsymbol  w}^{\mathrm{T}}\mathbf{X}^{\mathrm{T}}\boldsymbol{y}+\hat{\boldsymbol  w}^{\mathrm{T}}\mathbf{X}^{\mathrm{T}}\mathbf{X}\hat{\boldsymbol w}$$ 对$\hat{\boldsymbol w}$求导可得 $$\cfrac{\partial E_{\hat{\boldsymbol w}}}{\partial \hat{\boldsymbol  w}}&#x3D; \cfrac{\partial \boldsymbol{y}^{\mathrm{T}}\boldsymbol{y}}{\partial \hat{\boldsymbol w}}-\cfrac{\partial  \boldsymbol{y}^{\mathrm{T}}\mathbf{X}\hat{\boldsymbol w}}{\partial  \hat{\boldsymbol w}}-\cfrac{\partial \hat{\boldsymbol  w}^{\mathrm{T}}\mathbf{X}^{\mathrm{T}}\boldsymbol{y}}{\partial  \hat{\boldsymbol w}}+\cfrac{\partial \hat{\boldsymbol  w}^{\mathrm{T}}\mathbf{X}^{\mathrm{T}}\mathbf{X}\hat{\boldsymbol  w}}{\partial \hat{\boldsymbol w}}$$ 由&#x3D;&#x3D;矩阵微分公式&#x3D;&#x3D;$\cfrac{\partial\boldsymbol{a}^{\mathrm{T}}\boldsymbol{x}}{\partial\boldsymbol{x}}&#x3D;\cfrac{\partial\boldsymbol{x}^{\mathrm{T}}\boldsymbol{a}}{\partial\boldsymbol{x}}&#x3D;\boldsymbol{a},\cfrac{\partial\boldsymbol{x}^{\mathrm{T}}\mathbf{A}\boldsymbol{x}}{\partial\boldsymbol{x}}&#x3D;(\mathbf{A}+\mathbf{A}^{\mathrm{T}})\boldsymbol{x}$可得 $$\cfrac{\partial E_{\hat{\boldsymbol w}}}{\partial \hat{\boldsymbol  w}}&#x3D;  0-\mathbf{X}^{\mathrm{T}}\boldsymbol{y}-\mathbf{X}^{\mathrm{T}}\boldsymbol{y}+(\mathbf{X}^{\mathrm{T}}\mathbf{X}+\mathbf{X}^{\mathrm{T}}\mathbf{X})\hat{\boldsymbol w}$$ $$\cfrac{\partial E_{\hat{\boldsymbol w}}}{\partial \hat{\boldsymbol  w}}&#x3D;2\mathbf{X}^{\mathrm{T}}(\mathbf{X}\hat{\boldsymbol  w}-\boldsymbol{y})$$</p>
<h2 id="l-p-范数"><a href="#l-p-范数" class="headerlink" title="$l_p$范数"></a><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/26884695">$l_p$范数</a></h2><p><img src="/images/%E8%A5%BF%E7%93%9C%E4%B9%A6%E9%9A%8F%E7%AC%94.assets/image-20200719000648999.png" alt="image-20200719000648999"></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/Vancuicide/p/9982212.html">意义解释</a></p>
<h2 id="局部最优与全局最优"><a href="#局部最优与全局最优" class="headerlink" title="局部最优与全局最优"></a>局部最优与全局最优</h2><p>凸优化：（只是当下的理解）损失函数设置为一个凸函数，就可以实现保证局部最优就是全局最优，本质上是对于损失函数进行数学改造来避免梯度下降过程中未找到全局最优而陷入局部最优解。</p>
<h2 id="遗传算法、模拟退火"><a href="#遗传算法、模拟退火" class="headerlink" title="遗传算法、模拟退火"></a>遗传算法、模拟退火</h2><p><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/ae5157c26af9">参考博客</a></p>
<p>下面介绍介绍“袋鼠跳”的几种方式。</p>
<ul>
<li>爬山算法：一只袋鼠朝着比现在高的地方跳去。它找到了不远处的最高的山峰。但是这座山不一定是最高峰。这就是爬山算法，它不能保证局部最优值就是全局最优值。</li>
<li>模拟退火：袋鼠喝醉了。它随机地跳了很长时间。这期间，它可能走向高处，也可能踏入平地。但是，它渐渐清醒了并朝最高峰跳去。这就是模拟退火算法。</li>
<li>遗传算法：有很多袋鼠，它们降落到喜玛拉雅山脉的任意地方。这些袋鼠并不知道它们的任务是寻找珠穆朗玛峰。但每过几年，就在一些海拔高度较低的地方射杀一些袋鼠。于是，不断有袋鼠死于海拔较低的地方，而越是在海拔高的袋鼠越是能活得更久，也越有机会生儿育女。就这样经过许多年，这些袋鼠们竟然都不自觉地聚拢到了一个个的山峰上，可是在所有的袋鼠中，只有聚拢到珠穆朗玛峰的袋鼠被带回了美丽的澳洲。</li>
</ul>
<h2 id="熵的度量，信息熵"><a href="#熵的度量，信息熵" class="headerlink" title="熵的度量，信息熵"></a>熵的度量，信息熵</h2><p>视频P52-P54 + <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/22178202">知乎源视频</a></p>
<p><strong>熵的度量：一般分布  P54</strong>  具体解释了知乎视频里面对于概率取倒数内容的细致理解：原本的6个等可能情况在进入1&#x2F;2时（相当于可以被拆成3个等可能的1&#x2F;6），减少了一件3种等可能事件情况。（感觉概括的不大好，还是看原视频理解吧）</p>
<p>弹幕中的概括：因为一旦走了A的路线，那三种情况就是确定的了，而我们计算熵的时候用$log_26$，强行认为都是等可能的不确定，所以最后要扣除</p>
<p><img src="/images/%E8%A5%BF%E7%93%9C%E4%B9%A6%E9%9A%8F%E7%AC%94.assets/image-20200719235129644-1597756979419.png" alt="image-20200719235129644"></p>
<p><img src="/images/%E8%A5%BF%E7%93%9C%E4%B9%A6%E9%9A%8F%E7%AC%94.assets/image-20200719235333941.png" alt="image-20200719235333941"></p>
<h2 id="CART分类树算法"><a href="#CART分类树算法" class="headerlink" title="CART分类树算法"></a>CART分类树算法</h2><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/keye/p/10564914.html">参考博客1</a>(含实例，可结合理解)</p>
<pre><code>    **CART分类树算法对连续值的处理**，思想和C4.5相同，都是将连续的特征离散化。唯一区别在选择划分点时，C4.5是信息增益比，CART是基尼系数。
</code></pre>
<p>　　具体思路：m个样本的连续特征A有m个，从小到大排列a1，a2，……，am，则CART取相邻两样本值的平均数做划分点，一共取m-1个，其中第i个划分点Ti表示为：Ti &#x3D; (ai + ai+1)&#x2F;2。分别计算以这m-1个点作为二元分类点时的基尼系数。选择基尼系数最小的点为该连续特征的二元离散分类点。比如取到的基尼系数最小的点为at，则小于at的值为类别1，大于at的值为类别2，这样就做到了连续特征的离散化。</p>
<p>　　注意的是，与ID3、C4.5处理离散属性不同的是，如果当前节点为连续属性，则该属性在后面还可以参与子节点的产生选择过程。</p>
<p>　　<strong>CART分类树算法对离散值的处理</strong>，采用的思路：不停的二分离散特征。</p>
<p>　　在ID3、C4.5，特征A被选取建立决策树节点，如果它有3个类别A1,A2,A3，我们会在决策树上建立一个三叉点，这样决策树是多叉树。</p>
<p>　　CART采用的是不停的二分。会考虑把特征A分成{A1}和{A2,A3}、{A2}和{A1,A3}、{A3}和{A1,A2}三种情况，找到基尼系数最小的组合，比如{A2}和{A1,A3}，然后建立二叉树节点，一个节点是A2对应的样本，另一个节点是{A1,A3}对应的样本。由于这次没有把特征A的取值完全分开，后面还有机会对子节点继续选择特征A划分A1和A3。这和ID3、C4.5不同，在ID3或C4.5的一颗子树中，离散特征只会参与一次节点的建立。</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/lanyuelvyun/article/details/88697386">参考博客2</a></p>
<h2 id="KKT条件"><a href="#KKT条件" class="headerlink" title="KKT条件"></a>KKT条件</h2><p><strong>最简单的概括：&#x3D;&#x3D;一个不等式约束条件换三个式子&#x3D;&#x3D;</strong></p>
<p><img src="/images/%E8%A5%BF%E7%93%9C%E4%B9%A6%E9%9A%8F%E7%AC%94.assets/image-20200725202830087.png" alt="image-20200725202830087">    ——-&gt;    <img src="/images/%E8%A5%BF%E7%93%9C%E4%B9%A6%E9%9A%8F%E7%AC%94.assets/image-20200725202913557.png" alt="image-20200725202913557"></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/38163970">参考博客</a></p>
<p><strong>不等式约束优化问题</strong></p>
<p>接下来我们将约束等式 <img src="https://www.zhihu.com/equation?tex=g(%5Cmathbf%7Bx%7D)=0" alt="[公式]"> 推广为不等式 <img src="https://www.zhihu.com/equation?tex=g(%5Cmathbf%7Bx%7D)%5Cle+0" alt="[公式]"> 。考虑这个问题</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle+%5Cbegin%7Barray%7D%7Bll%7D+%5Chbox%7Bmin%7D&f(%5Cmathbf%7Bx%7D)%5C%5C+%5Chbox%7Bs.t.%7D&g(%5Cmathbf%7Bx%7D)%5Cle+0.+%5Cend%7Barray%7D%5C%5C" alt="[公式]"><br>约束不等式 <img src="https://www.zhihu.com/equation?tex=g(%5Cmathbf%7Bx%7D)%5Cle+0" alt="[公式]"> 称为原始可行性(primal feasibility)，据此我们定义可行域(feasible region) <img src="https://www.zhihu.com/equation?tex=K=%7B%5Cmathbf%7Bx%7D%5Cin%5Cmathbb%7BR%7D%5En%7Cg(%5Cmathbf%7Bx%7D)%5Cle+0%7D" alt="[公式]"> 。假设 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7Bx%7D%5E%5Cstar" alt="[公式]"> 为满足约束条件的最佳解，分开两种情况讨论：</p>
<p>(1) <img src="https://www.zhihu.com/equation?tex=g(%5Cmathbf%7Bx%7D%5E%5Cstar)%3C0" alt="[公式]"> ，最佳解位于 <img src="https://www.zhihu.com/equation?tex=K" alt="[公式]"> 的内部，称为内部解(interior solution)，这时约束条件是无效的(inactive)；</p>
<p>(2) <img src="https://www.zhihu.com/equation?tex=g(%5Cmathbf%7Bx%7D%5E%5Cstar)=0" alt="[公式]"> ，最佳解落在 <img src="https://www.zhihu.com/equation?tex=K" alt="[公式]"> 的边界，称为边界解(boundary solution)，此时约束条件是有效的(active)。</p>
<p>这两种情况的最佳解具有不同的必要条件。</p>
<p>(1)内部解：在约束条件无效的情形下， <img src="https://www.zhihu.com/equation?tex=g(%5Cmathbf%7Bx%7D)" alt="[公式]"> 不起作用，约束优化问题退化为无约束优化问题，因此驻点 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7Bx%7D%5E%5Cstar" alt="[公式]"> 满足 <img src="https://www.zhihu.com/equation?tex=%5Cnabla+f+=%5Cmathbf%7B0%7D" alt="[公式]"> 且 <img src="https://www.zhihu.com/equation?tex=%5Clambda=0" alt="[公式]"> 。</p>
<p>(2)边界解：在约束条件有效的情形下，约束不等式变成等式 <img src="https://www.zhihu.com/equation?tex=g(%5Cmathbf%7Bx%7D)=0" alt="[公式]"> ，这与前述Lagrange乘数法的情况相同。我们可以证明驻点 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7Bx%7D%5E%5Cstar" alt="[公式]"> 发生于 <img src="https://www.zhihu.com/equation?tex=%5Cnabla+f%5Cin%5Chbox%7Bspan%7D%7B%5Cnabla+g%7D" alt="[公式]"> ，换句话说，存在 <img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="[公式]"> 使得 <img src="https://www.zhihu.com/equation?tex=%5Cnabla+f=-%5Clambda%5Cnabla+g" alt="[公式]"> ，但这里 <img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="[公式]"> 的正负号是有其意义的。<strong>因为我们希望最小化</strong> <img src="https://www.zhihu.com/equation?tex=f" alt="[公式]"> <strong>，梯度</strong> <img src="https://www.zhihu.com/equation?tex=%5Cnabla+f" alt="[公式]"> <strong>(函数</strong> <img src="https://www.zhihu.com/equation?tex=f" alt="[公式]"> <strong>在点</strong> <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7Bx%7D" alt="[公式]"> <strong>的最陡上升方向)应该指向可行域</strong> <img src="https://www.zhihu.com/equation?tex=K" alt="[公式]"> <strong>的内部(因为你的最优解最小值是在边界取得的)，但</strong> <img src="https://www.zhihu.com/equation?tex=%5Cnabla+g" alt="[公式]"> <strong>指向</strong> <img src="https://www.zhihu.com/equation?tex=K" alt="[公式]"> <strong>的外部(即</strong> <img src="https://www.zhihu.com/equation?tex=g(%5Cmathbf%7Bx%7D)%3E0" alt="[公式]"> <strong>的区域，因为你的约束是小于等于0)，因此</strong> <img src="https://www.zhihu.com/equation?tex=%5Clambda%5Cge+0" alt="[公式]"> **，称为对偶可行性(dual feasibility)**。&#x3D;&#x3D;（f：想象一个三维空间中凸出来的情形；g：想象一个三维空间中凹下去的情形）&#x3D;&#x3D;（<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV17J411C7zZ?p=83">相关视频</a>）</p>
<p>因此，不论是内部解或边界解， <img src="https://www.zhihu.com/equation?tex=%5Clambda+g(%5Cmathbf%7Bx%7D)=0" alt="[公式]"> 恒成立，称为互补松弛性(complementary slackness)。整合上述两种情况，最佳解的必要条件包括Lagrangian函数 <img src="https://www.zhihu.com/equation?tex=L(%5Cmathbf%7Bx%7D,%5Clambda)" alt="[公式]"> 的定常方程式、原始可行性、对偶可行性，以及互补松弛性：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle+%5Cbegin%7Baligned%7D+%5Cnabla_%7B%5Cmathbf%7Bx%7D%7DL&=%5Cnabla+f+%5Clambda%5Cnabla+g=%5Cmathbf%7B0%7D%5C%5C+g(%5Cmathbf%7Bx%7D)&%5Cle+0%5C%5C+%5Clambda&+%5Cge+0%5C%5C+%5Clambda+g(%5Cmathbf%7Bx%7D)&=0.+%5Cend%7Baligned%7D%5C%5C" alt="[公式]"><br>这些条件合称为Karush-Kuhn-Tucker (KKT)条件。如果我们要最大化 <img src="https://www.zhihu.com/equation?tex=f(%5Cmathbf%7Bx%7D)" alt="[公式]"> 且受限于 <img src="https://www.zhihu.com/equation?tex=g(%5Cmathbf%7Bx%7D)%5Cle+0" alt="[公式]"> ，那么对偶可行性要改成 <img src="https://www.zhihu.com/equation?tex=%5Clambda%5Cle+0" alt="[公式]"> 。</p>
<p>上面结果可推广至多个约束等式与约束不等式的情况。考虑标准约束优化问题(或称非线性规划)：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle+%5Cbegin%7Barray%7D%7Blll%7D+%5Chbox%7Bmin%7D&f(%5Cmathbf%7Bx%7D)%5C%5C+%5Chbox%7Bs.t.%7D&g_j(%5Cmathbf%7Bx%7D)=0,&j=1,%5Cldots,m+,%5C%5C+&h_k(%5Cmathbf%7Bx%7D)%5Cle+0,&k=1,%5Cldots,p.+%5Cend%7Barray%7D%5C%5C" alt="[公式]"> </p>
<p>定义Lagrangian 函数</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle+L%5Cleft(%5Cmathbf%7Bx%7D,%5C%7B%5Clambda_j%5C%7D,%5C%7B%5Cmu_k%5C%7D%5Cright)=f(%5Cmathbf%7Bx%7D)+%5Csum_%7Bj=1%7D%5Em%5Clambda_jg_j(+%5Cmathbf%7Bx%7D)+%5Csum_%7Bk=1%7D%5Ep%5Cmu_kh_k(%5Cmathbf%7Bx%7D)%5C%5C" alt="[公式]"><br>其中 <img src="https://www.zhihu.com/equation?tex=%5Clambda_j" alt="[公式]"> 是对应 <img src="https://www.zhihu.com/equation?tex=g_j(%5Cmathbf%7Bx%7D)=0" alt="[公式]"> 的Lagrange乘数， <img src="https://www.zhihu.com/equation?tex=%5Cmu_k" alt="[公式]"> $是对应 <img src="https://www.zhihu.com/equation?tex=h_k(%5Cmathbf%7Bx%7D)%5Cle+0" alt="[公式]"> 的Lagrange乘数(或称KKT乘数)。KKT条件包括</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle+%5Cbegin%7Baligned%7D+%5Cnabla_%7B%5Cmathbf%7Bx%7D%7DL&=%5Cmathbf%7B0%7D%5C%5C+g_j(%5Cmathbf%7Bx%7D)&=0,~~j=1,%5Cldots,m,%5C%5C+h_k(%5Cmathbf%7Bx%7D)&%5Cle+0,%5C%5C+%5Cmu_k&%5Cge+0,%5C%5C+%5Cmu_k+h_k(%5Cmathbf%7Bx%7D)&=0,~~k=1,%5Cldots,p.+%5Cend+%7Baligned%7D%5C%5C" alt="[公式]"> </p>
<h2 id="支持向量机"><a href="#支持向量机" class="headerlink" title="支持向量机"></a>支持向量机</h2><p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV17J411C7zZ?p=74">图6.1的理解</a>：+和-相当于是第三维上的属性y（隐含了一条轴），通过$x_1$和$x_2$确定第三维的属性，$x_1$和$x_2$在一起相当于是（x,y）里面的$x$，是一个向量，包含两个维度，数值分别对应$x_1$和$x_2$。</p>
<p><strong>向量表示符号的差别：分号间隔表示列向量，逗号间隔表示横向量</strong></p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV17J411C7zZ?p=79">拉格朗日乘数法的应用由来与理解</a></p>
<p>公式6.11推导：</p>
<p>$$\begin{aligned} \max_{\boldsymbol{\alpha}} &amp; \sum_{i&#x3D;1}^m\alpha_i -  \frac{1}{2}\sum_{i &#x3D; 1}^m\sum_{j&#x3D;1}^m\alpha_i \alpha_j  y_iy_j\boldsymbol{x}<em>i^T\boldsymbol{x}<em>j \ \text { s.t. } &amp; \sum</em>{i&#x3D;1}^m \alpha_i y_i &#x3D;0 \ &amp; \alpha_i \geq 0 \quad i&#x3D;1,2,\dots ,m \end{aligned}$$<br> [推导]：将公式(6.9)和公式(6.10)代入公式(6.8)即可将$L(\boldsymbol{w},b,\boldsymbol{\alpha})$中的$\boldsymbol{w}$和$b$消去，再考虑公式(6.10)的约束，就得到了公式(6.6)的对偶问题 $$\begin{aligned} \inf</em>{\boldsymbol{w},b} L(\boldsymbol{w},b,\boldsymbol{\alpha})   &amp;&#x3D;\frac{1}{2}\boldsymbol{w}^T\boldsymbol{w}+\sum_{i&#x3D;1}^m\alpha_i  -\sum_{i&#x3D;1}^m\alpha_iy_i\boldsymbol{w}^T\boldsymbol{x}<em>i-\sum</em>{i&#x3D;1}^m\alpha_iy_ib \ &amp;&#x3D;\frac {1}{2}\boldsymbol{w}^T\sum  <em>{i&#x3D;1}^m\alpha_iy_i\boldsymbol{x}<em>i-\boldsymbol{w}^T\sum  <em>{i&#x3D;1}^m\alpha_iy_i\boldsymbol{x}<em>i+\sum</em>{i&#x3D;1}^m\alpha_i -b\sum <em>{i&#x3D;1}^m\alpha_iy_i \ &amp; &#x3D; -\frac {1}{2}\boldsymbol{w}^T\sum <em>{i&#x3D;1}^m\alpha_iy_i\boldsymbol{x}<em>i+\sum</em>{i&#x3D;1}^m\alpha_i -b\sum <em>{i&#x3D;1}^m\alpha_iy_i \end{aligned}$$ 由于$\sum\limits</em>{i&#x3D;1}^{m}\alpha_iy_i&#x3D;0$，所以上式最后一项可化为0，于是得 $$\begin{aligned} \inf</em>{\boldsymbol{w},b} L(\boldsymbol{w},b,\boldsymbol{\alpha})  &amp;&#x3D; -\frac {1}{2}\boldsymbol{w}^T\sum  <em>{i&#x3D;1}^m\alpha_iy_i\boldsymbol{x}<em>i+\sum</em>{i&#x3D;1}^m\alpha_i \ &amp;&#x3D;-\frac {1}{2}(\sum</em>{i&#x3D;1}^{m}\alpha_iy_i\boldsymbol{x}<em>i)^T(\sum <em>{i&#x3D;1}^m\alpha_iy_i\boldsymbol{x}<em>i)+\sum</em>{i&#x3D;1}^m\alpha_i \ &amp;&#x3D;-\frac {1}{2}\sum</em>{i&#x3D;1}^{m}\alpha_iy_i\boldsymbol{x}<em>i^T\sum</em>{i&#x3D;1}^m\alpha_iy_i\boldsymbol{x}<em>i+\sum</em>{i&#x3D;1}^m\alpha_i \ &amp;&#x3D;\sum</em>{i&#x3D;1}^m\alpha_i-\frac {1}{2}\sum</em>{i&#x3D;1 }^{m}\sum</em>{j&#x3D;1}^{m}\alpha_i\alpha_jy_iy_j\boldsymbol{x}<em>i^T\boldsymbol{x}<em>j \end{aligned}$$ 所以 $$\max</em>{\boldsymbol{\alpha}}\inf</em>{\boldsymbol{w},b} L(\boldsymbol{w},b,\boldsymbol{\alpha})&#x3D;\max</em>{\boldsymbol{\alpha}} \sum</em>{i&#x3D;1}^m\alpha_i - \frac{1}{2}\sum_{i &#x3D; 1}^m\sum_{j&#x3D;1}^m\alpha_i \alpha_j y_iy_j\boldsymbol{x}_i^T\boldsymbol{x}_j $$</p>
<h3 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h3><p><strong>高斯核函数</strong>：<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV17J411C7zZ?p=90">推导过程</a>，其中最后代入s的一步应该理解并不准确（不应该是拆成根号的情况，而是应该拆成相乘 的两项），参考下图，拆分出来的两个高维向量应当只包含x或者只包含y，这样才能对应上&#x3D;&#x3D;$\phi(x_i)^T\phi(x_j)$&#x3D;&#x3D;的形式。</p>
<p><img src="/images/%E8%A5%BF%E7%93%9C%E4%B9%A6%E9%9A%8F%E7%AC%94.assets/image-20200726230331125.png" alt="image-20200726230331125"></p>
<h2 id="经验风险、期望风险、结构风险"><a href="#经验风险、期望风险、结构风险" class="headerlink" title="经验风险、期望风险、结构风险"></a>经验风险、期望风险、结构风险</h2><p><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/903e35e1c95a">解释博客</a></p>
<p><strong>为何引入结构化风险？</strong></p>
<p>虽然可以使用经验损失近似估计期望风险，但是大数定理的前提是N无穷大，实际上，我们的训练集一般不会特别大，此时就需要对经验风险做出适当调整才能近似估计。因此引入结构风险。</p>
<p>结构化风险是为了缓解数据集过小而导致的过拟合现象，其等价于正则化，本质上反应的是模型的复杂度。认为经验风险越小，参数越多，模型越复杂，因此引入对模型复杂度的惩罚机制。</p>
<p><strong>当模型是条件概率分布、损失函数是对数损失函数、模型复杂度由模型的先验概率表示时，结构风险最小化就等价于最大后验概率估计。</strong></p>
<h2 id="贝叶斯定理"><a href="#贝叶斯定理" class="headerlink" title="贝叶斯定理"></a>贝叶斯定理</h2><p><img src="/images/%E8%A5%BF%E7%93%9C%E4%B9%A6%E9%9A%8F%E7%AC%94.assets/image-20200801205139517.png" alt="image-20200801205139517"></p>
<p><a target="_blank" rel="noopener" href="https://www.matongxue.com/madocs/279.html">解释博客</a></p>
<p><strong>重要条件：朴素贝叶斯算法是假设各个特征之间相互独立</strong>  <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/26262151">原因解释及实例</a>  表现形式：多个属性可由连乘来计算</p>
<p><img src="/images/%E8%A5%BF%E7%93%9C%E4%B9%A6%E9%9A%8F%E7%AC%94.assets/image-20200801225003536.png" alt="image-20200801225003536"></p>
<p><img src="/images/%E8%A5%BF%E7%93%9C%E4%B9%A6%E9%9A%8F%E7%AC%94.assets/image-20200801230710751.png" alt="image-20200801230710751"></p>
<h2 id="朴素贝叶斯法处理连续及离散属性"><a href="#朴素贝叶斯法处理连续及离散属性" class="headerlink" title="朴素贝叶斯法处理连续及离散属性"></a>朴素贝叶斯法处理连续及离散属性</h2><p><a target="_blank" rel="noopener" href="https://www.pianshen.com/article/98701166125/">关于连续属性处理的实例</a></p>
<p><img src="/images/%E8%A5%BF%E7%93%9C%E4%B9%A6%E9%9A%8F%E7%AC%94.assets/image-20200802130922498.png" alt="image-20200802130922498"></p>
<p>上面那个博客这个地方写错了，是没有平方的，对应书上的式7.18</p>
<p><strong>疑问：</strong>连续属性为何能使用概率密度去乘  （<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/51992999">相关解释</a>）</p>
<p><strong>解答：</strong></p>
<p>作者：342788<br>链接：<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/51992999/answer/248807822">https://www.zhihu.com/question/51992999/answer/248807822</a><br>来源：知乎</p>
<p>当特征是连续变量的时候，运用多项式模型就会导致很多$P(x_{i}|y_{k})&#x3D;0$（不做平滑的情况下），此时即使做平滑，所得到的条件概率也难以描述真实情况。所以处理连续的特征变量，应该采用高斯模型。</p>
<p>通过一个例子来说明： <a href="https://link.zhihu.com/?target=http://www.ruanyifeng.com/blog/2013/12/naive_bayes_classifier.html">性别分类的例子</a> <a href="https://link.zhihu.com/?target=https://en.wikipedia.org/wiki/Naive_Bayes_classifier%23Sex_classification">来自维基</a></p>
<p>下面是一组人类身体特征的统计资料。</p>
<table>
<thead>
<tr>
<th align="center">性别</th>
<th align="center">身高（英尺）</th>
<th align="center">体重（磅）</th>
<th align="center">脚掌（英寸）</th>
</tr>
</thead>
<tbody><tr>
<td align="center">男</td>
<td align="center">6</td>
<td align="center">180</td>
<td align="center">12</td>
</tr>
<tr>
<td align="center">男</td>
<td align="center">5.92</td>
<td align="center">190</td>
<td align="center">11</td>
</tr>
<tr>
<td align="center">男</td>
<td align="center">5.58</td>
<td align="center">170</td>
<td align="center">12</td>
</tr>
<tr>
<td align="center">男</td>
<td align="center">5.92</td>
<td align="center">165</td>
<td align="center">10</td>
</tr>
<tr>
<td align="center">女</td>
<td align="center">5</td>
<td align="center">100</td>
<td align="center">6</td>
</tr>
<tr>
<td align="center">女</td>
<td align="center">5.5</td>
<td align="center">150</td>
<td align="center">8</td>
</tr>
<tr>
<td align="center">女</td>
<td align="center">5.42</td>
<td align="center">130</td>
<td align="center">7</td>
</tr>
<tr>
<td align="center">女</td>
<td align="center">5.75</td>
<td align="center">150</td>
<td align="center">9</td>
</tr>
</tbody></table>
<p>已知某人身高6英尺、体重130磅，脚掌8英寸，请问该人是男是女？ 根据朴素贝叶斯分类器，计算下面这个式子的值。</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">P(身高|性别) x P(体重|性别) x P(脚掌|性别) x P(性别)</span><br></pre></td></tr></table></figure>

<p>这里的困难在于，由于身高、体重、脚掌都是连续变量，不能采用离散变量的方法计算概率。而且由于样本太少，所以也无法分成区间计算。怎么办？ 这时，可以假设男性和女性的身高、体重、脚掌都是正态分布，通过样本计算出均值和方差，也就是得到正态分布的密度函数。有了密度函数，就可以把值代入，算出某一点的密度函数的值。 比如，男性的身高是均值5.855、方差0.035的正态分布。所以，男性的身高为6英尺的概率的相对值等于1.5789（大于1并没有关系，因为这里是密度函数的值，只用来反映各个值的相对可能性）。</p>
<p><img src="https://picb.zhimg.com/50/v2-77bae98b689babf23c023354fcb7ae28_hd.jpg?source=1940ef5c" alt="img"><img src="https://picb.zhimg.com/80/v2-77bae98b689babf23c023354fcb7ae28_720w.jpg?source=1940ef5c" alt="img"></p>
<p>对于脚掌和体重同样可以计算其均值与方差。有了这些数据以后，就可以计算性别的分类了。</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">P(身高=6|男) x P(体重=130|男) x P(脚掌=8|男) x P(男) </span><br><span class="line">　　　　= 6.1984 x e-9</span><br><span class="line">　　P(身高=6|女) x P(体重=130|女) x P(脚掌=8|女) x P(女) </span><br><span class="line">　　　　= 5.3778 x e-4</span><br></pre></td></tr></table></figure>

<p>可以看到，女性的概率比男性要高出将近10000倍，所以判断该人为女性。</p>
<p><strong>对解答大概概括：</strong>这里用概率密度只是为了表现出其位于附近的可能性大小，最终比较的时候是通过两种类别的大小比较来判断的，并不需要直接使用准确的概率去量化描述，只要使用相同的方式能够体现出可能性大小的差异就行。（<strong>如果离散属性和连续属性都有，虽然连续属性会造成量纲的不同，但是互相之间的倍率关系还是相同的。1和5对上0.1和0.5，都是5倍关系，式子里面都是乘，不会影响比较。或者也可以这么理解，把两个东西归一化到0~1直接，相当于两个同时除了一个常数，两个式子同时除一个常数不会影响比较。</strong>）</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/lyl771857509/article/details/78993493">半朴素贝叶斯分类实例</a></p>
<p><img src="/images/%E8%A5%BF%E7%93%9C%E4%B9%A6%E9%9A%8F%E7%AC%94.assets/image-20200802194534174.png" alt="image-20200802194534174">  </p>
<p><strong>此处标签里的“否”对应的就是下方的“一般”。</strong></p>
<p><img src="/images/%E8%A5%BF%E7%93%9C%E4%B9%A6%E9%9A%8F%E7%AC%94.assets/image-20200802194442631.png" alt="image-20200802194442631"></p>
<p><img src="/images/%E8%A5%BF%E7%93%9C%E4%B9%A6%E9%9A%8F%E7%AC%94.assets/image-20200802194410813.png" alt="image-20200802194410813"></p>
<h2 id="吉布斯采样"><a href="#吉布斯采样" class="headerlink" title="吉布斯采样"></a>吉布斯采样</h2><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/pipisorry/article/details/51373090">博客</a></p>
<h3 id="为什么要用吉布斯采样"><a href="#为什么要用吉布斯采样" class="headerlink" title="为什么要用吉布斯采样"></a>为什么要用吉布斯采样</h3><h4 id="什么是sampling"><a href="#什么是sampling" class="headerlink" title="什么是sampling?"></a>什么是sampling?</h4><p>sampling就是以一定的概率分布，看发生什么事件。举一个例子。甲只能E：吃饭、学习、打球，时间T：上午、下午、晚上，天气W：晴朗、刮风、下雨。现在要一个sample，这个sample可以是：打球+下午+晴朗。</p>
<h4 id="吉布斯采样的通俗解释？"><a href="#吉布斯采样的通俗解释？" class="headerlink" title="吉布斯采样的通俗解释？"></a>吉布斯采样的通俗解释？</h4><p>问题是我们不知道p(E,T,W)，或者说，不知道三件事的联合分布joint distribution。当然，如果知道的话，就没有必要用gibbs sampling了。但是，我们知道三件事的conditional distribution。也就是说，p(E|T,W),p(T|E,W),p(W|E,T)。现在要做的就是通过这三个已知的条件分布，再用gibbs sampling的方法，得到联合分布。<br> 具体方法。首先随便初始化一个组合,i.e.  学习+晚上+刮风，然后依条件概率改变其中的一个变量。具体说，假设我们知道晚上+刮风，我们给E生成一个变量，比如，学习-》吃饭。我们再依条件概率改下一个变量，根据学习+刮风，把晚上变成上午。类似地，把刮风变成刮风（当然可以变成相同的变量）。这样学习+晚上+刮风-》吃饭+上午+刮风。同样的方法，得到一个序列，每个单元包含三个变量，也就是一个马尔可夫链。然后跳过初始的一定数量的单元（比如100个），然后隔一定的数量取一个单元（比如隔20个取1个）。这样sample到的单元，是逼近联合分布的。</p>
<h2 id="贝叶斯网"><a href="#贝叶斯网" class="headerlink" title="贝叶斯网"></a>贝叶斯网</h2><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_41575207/article/details/82077800">博客1</a>：主要讲得是贝叶斯网的结构</p>
<p><img src="/images/%E8%A5%BF%E7%93%9C%E4%B9%A6%E9%9A%8F%E7%AC%94.assets/image-20200803015800838.png" alt="image-20200803015800838"></p>
<h2 id="EM算法"><a href="#EM算法" class="headerlink" title="EM算法"></a>EM算法</h2><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/36331115">博客</a></p>
<p>E步：用参数推测（隐变量）值</p>
<p>M步：计算参数</p>
<p>EM的意思是“<strong>Expectation Maximization</strong>”，具体方法为：</p>
<ul>
<li>先设定男生和女生的身高分布参数(初始值)，例如男生的身高分布为 <img src="https://www.zhihu.com/equation?tex=N(%5Cmu_1+=+172,+%5Csigma%5E2_1=5%5E2)" alt="[公式]"> ， 女生的身高分布为 <img src="https://www.zhihu.com/equation?tex=N(%5Cmu_2+=+162,+%5Csigma%5E2_2=5%5E2)" alt="[公式]"> ，当然了，刚开始肯定没那么准；</li>
<li>然后计算出每个人更可能属于第一个还是第二个正态分布中的（例如，这个人的身高是180，那很明显，他极大可能属于男生），这个是属于Expectation 一步；</li>
<li>我们已经大概地按上面的方法将这 200 个人分为男生和女生两部分，我们就可以根据之前说的极大似然估计分别对男生和女生的身高分布参数进行估计（这不变成了<strong>极大</strong>似然估计了吗？<strong>极大即为Maximization</strong>）这步称为 Maximization；</li>
<li>然后，当我们更新这两个分布的时候，每一个学生属于女生还是男生的概率又变了，那么我们就再需要调整E步；</li>
<li>……如此往复，直到参数基本不再发生变化或满足结束条件为止。</li>
</ul>
<h2 id="AdaBoost"><a href="#AdaBoost" class="headerlink" title="AdaBoost"></a>AdaBoost</h2><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/27126737">博客</a></p>
<p>工作原理：通过每次降低个体学习器的分类误差，加大效果好的个体学习器的重要性，得到最终的集成学习器。</p>
<h2 id="Boosting与Bagging"><a href="#Boosting与Bagging" class="headerlink" title="Boosting与Bagging"></a>Boosting与Bagging</h2><p>从偏差-方差分解的角度看，Boosting主要关注降低偏差，因此Boosting能基于泛化性能相当弱的学习器构建出很强的集成。</p>
<p>从偏差-方差的角度看，Bagging主要关注降低方差，因此它在不剪枝决策树、神经网络等易受样本扰动的学习器上效用更为明显。</p>
<h2 id="偏差-方差的平衡"><a href="#偏差-方差的平衡" class="headerlink" title="偏差-方差的平衡"></a>偏差-方差的平衡</h2><ul>
<li>方差一般会因为训练样本的增加而减少</li>
<li>选择能力强的模型来减少偏差</li>
<li>训练样本通常数量不够，二者无法兼顾</li>
</ul>
<h2 id="偏差-方差分解的平衡"><a href="#偏差-方差分解的平衡" class="headerlink" title="偏差-方差分解的平衡"></a>偏差-方差分解的平衡</h2><ul>
<li>选择能力强的模型减少偏差<ul>
<li>在训练集上要足够低的误差</li>
</ul>
</li>
<li>减少模型的方差<ul>
<li>增加训练集的样本数</li>
<li>正则化</li>
</ul>
</li>
</ul>
<h2 id="高斯混合聚类"><a href="#高斯混合聚类" class="headerlink" title="高斯混合聚类"></a>高斯混合聚类</h2><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/lotusng/article/details/79990724">博客1</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/sptv-88/p/11697337.html">多元高斯分布</a></p>
<p><strong>&#x3D;&#x3D;公式(9.29)的说明：&#x3D;&#x3D;</strong></p>
<ul>
<li><p>首先，当<em>i</em>&#x3D;1，比如这里是指坏瓜类，那么这时坏瓜的高斯曲线已知已确定了（即$μ_1$、$Σ_1$已确定）。根据这个确定高斯曲线可以得到样本瓜<em>x</em>在坏瓜类中存在的概率<em>p</em>(<em>x</em>∣$μ_1$,$Σ_1$)。</p>
</li>
<li><p>然后， <em>p</em>(<em>x</em>∣$μ_1$,$Σ_1$) 与 $α_i$相乘的结果就是从坏瓜类中抽中样本瓜<em>x</em>的概率。（乘完的结果应该是相对于全部样本的概率）</p>
</li>
<li><p>最后，分别计算<em>i</em>&#x3D;1（从坏瓜类中抽）、<em>i</em>&#x3D;2（从一般瓜类中抽）和<em>i</em>&#x3D;3（从好瓜类中抽）的情况下抽到样本瓜<em>x</em>的概率，将这三种情况下的概率相加，得到的$P_m(x)$即是在自然界中抽一个瓜正好抽中样本瓜<em>x</em>的概率。</p>
</li>
</ul>
<p>由于每类瓜分别拥有3个参数$α$、$μ$、$Σ$，因为类<em>i</em>&#x3D;3，即这个例子里一共有9个参数。</p>
<p><strong>总结高斯混合聚类的步骤</strong>：首先假设样本集具有一些规律，包括可以以<em>α</em>参数作为比例分为<em>k</em>类且每类内符合高斯分布。然后根据贝叶斯原理利用极大似然法同时求出决定分类比例的<em>α</em>和决定类内高斯分布的<em>μ</em>、Σ。最后将样本根据<em>α</em>、<em>μ</em>、Σ再次通过贝叶斯原理求出样本该分在哪个簇。</p>
<p>整个步骤下来，这种做法其实就是一种<strong>原型聚类</strong>：通过找到可以刻画样本的原型（<em>α</em>、<em>μ</em>、Σ参数），迭代得到<em>α</em>、<em>μ</em>、Σ参数的最优解。</p>
<p><strong>公式9.33推导：</strong></p>
<p>$$ \sum_{j&#x3D;1}^{m} \frac{\alpha_{i} \cdot p\left(\boldsymbol{x}<em>{j} | \boldsymbol{\mu}</em>{i}, \boldsymbol{\Sigma}<em>{i}\right)}{\sum</em>{l&#x3D;1}^{k} \alpha_{l} \cdot p\left(\boldsymbol{x}<em>{j} | \boldsymbol{\mu}</em>{l}, \boldsymbol{\Sigma}<em>{l}\right)}\left(\boldsymbol{x}</em>{j}-\boldsymbol{\mu}<em>{i}\right)&#x3D;0 $$ [推导]：根据公式(9.28)可知： $$ p\left(\boldsymbol{x}</em>{j} | \boldsymbol{\mu}<em>{i}, \boldsymbol{\Sigma}</em>{i}\right)&#x3D;\frac{1}{(2 \pi)^{\frac{n}{2}}\left|\boldsymbol{\Sigma}<em>{i}\right|^{\frac{1}{2}}} \exp \left(-\frac{1}{2}\left(\boldsymbol{x}</em>{j}-\boldsymbol{\mu}<em>{i}\right)^{T} \boldsymbol{\Sigma}</em>{i}^{-1}\left(\boldsymbol{x}<em>{j}-\boldsymbol{\mu}</em>{i}\right)\right) $$ 又根据公式(9.32)，由 $$ \frac{\partial L L(D)}{\partial \boldsymbol{\mu}<em>{i}}&#x3D;\frac{\partial L L(D)}{\partial p\left(\boldsymbol{x}</em>{j} | \boldsymbol{\mu}<em>{i}, \boldsymbol{\Sigma}</em>{i}\right)} \cdot \frac{\partial p\left(\boldsymbol{x}<em>{j} | \boldsymbol{\mu}</em>{i}, \boldsymbol{\Sigma}<em>{i}\right)}{\partial \boldsymbol{\mu}</em>{i}}&#x3D;0 $$ 其中： $$ \begin{aligned} \frac{\partial L L(D)}{\partial p\left(\boldsymbol{x}<em>{j} | \boldsymbol{\mu}</em>{i}, \mathbf{\Sigma}<em>{i}\right)} &amp;&#x3D;\frac{\partial \sum</em>{j&#x3D;1}^{m} \ln \left(\sum_{l&#x3D;1}^{k} \alpha_{l} \cdot p\left(\boldsymbol{x}<em>{j} | \boldsymbol{\mu}</em>{l}, \boldsymbol{\Sigma}<em>{l}\right)\right)}{\partial p\left(\boldsymbol{x}</em>{j} | \boldsymbol{\mu}<em>{i}, \boldsymbol{\Sigma}</em>{i}\right)} \ &amp;&#x3D;\sum_{j&#x3D;1}^{m} \frac{\partial \ln \left(\sum_{l&#x3D;1}^{k} \alpha_{l} \cdot p\left(\boldsymbol{x}<em>{j} | \boldsymbol{\mu}</em>{l}, \boldsymbol{\Sigma}<em>{l}\right)\right)}{\partial p\left(\boldsymbol{x}</em>{j} | \boldsymbol{\mu}<em>{i}, \boldsymbol{\Sigma}</em>{i}\right)} \ &amp;&#x3D;\sum_{j&#x3D;1}^{m} \frac{\alpha_{i}}{\sum_{l&#x3D;1}^{k} \alpha_{l} \cdot p\left(\boldsymbol{x}<em>{j} | \boldsymbol{\mu}</em>{l}, \boldsymbol{\Sigma}_{l}\right)} \end{aligned} $$</p>
<p>$$ \begin{aligned} \frac{\partial p\left(\boldsymbol{x}<em>{j} | \boldsymbol{\mu}</em>{i}, \boldsymbol{\Sigma}<em>{i}\right)}{\partial \boldsymbol{\mu}</em>{i}} &amp;&#x3D;\frac{\partial \frac{1}{(2  \pi)^{\frac{n}{2}}\left|\Sigma_{i}\right|^{\frac{1}{2}}}  \exp\left({-\frac{1}{2}\left(\boldsymbol{x}<em>{j}-\boldsymbol{\mu}</em>{i}\right)^{\top}\boldsymbol{\Sigma}<em>{i}^{-1}\left(\boldsymbol{x}</em>{j}-\boldsymbol{\mu}<em>{i}\right)}\right)}{\partial \boldsymbol{\mu}</em>{i}} \ &amp;&#x3D;\frac{1}{(2 \pi)^{\frac{n}{2}}\left|\boldsymbol{\Sigma}<em>{i}\right|^{\frac{1}{2}}} \cdot \frac{\partial \exp\left({-\frac{1}{2}\left(\boldsymbol{x}</em>{j}-\boldsymbol{\mu}<em>{i}\right)^{\top} \boldsymbol{\Sigma}</em>{i}^{-1}\left(\boldsymbol{x}<em>{j}-\boldsymbol{\mu}</em>{i}\right)}\right)}{\partial \boldsymbol{\mu}<em>{i}}\ &amp;&#x3D;\frac{1}{(2 \pi)^{\frac{n}{2}}\left|\boldsymbol{\Sigma}</em>{i}\right|^{\frac{1}{2}}}\cdot \exp\left({-\frac{1}{2}\left(\boldsymbol{x}<em>{j}-\boldsymbol{\mu}</em>{i}\right)^{\top} \boldsymbol{\Sigma}<em>{i}^{-1}\left(\boldsymbol{x}</em>{j}-\boldsymbol{\mu}<em>{i}\right)}\right) \cdot-\frac{1}{2} \frac{\partial\left(\boldsymbol{x}</em>{j}-\boldsymbol{\mu}<em>{i}\right)^{\top} \boldsymbol{\Sigma}</em>{i}^{-1}\left(\boldsymbol{x}<em>{j}-\boldsymbol{\mu}</em>{i}\right)}{\partial \boldsymbol{\mu}<em>{i}}\ &amp;&#x3D;\frac{1}{(2 \pi)^{\frac{n}{2}}\left|\boldsymbol{\Sigma}</em>{i}\right|^{\frac{1}{2}}}\cdot \exp\left({-\frac{1}{2}\left(\boldsymbol{x}<em>{j}-\boldsymbol{\mu}</em>{i}\right)^{\top} \boldsymbol{\Sigma}<em>{i}^{-1}\left(\boldsymbol{x}</em>{j}-\boldsymbol{\mu}<em>{i}\right)}\right) \cdot\boldsymbol{\Sigma}</em>{i}^{-1}\left(\boldsymbol{x}<em>{j}-\boldsymbol{\mu}</em>{i}\right)\ &amp;&#x3D;p\left(\boldsymbol{x}<em>{j} | \boldsymbol{\mu}</em>{i}, \boldsymbol{\Sigma}<em>{i}\right) \cdot \boldsymbol{\Sigma}</em>{i}^{-1}\left(\boldsymbol{x}<em>{j}-\boldsymbol{\mu}</em>{i}\right) \end{aligned} $$</p>
<p>其中，由矩阵求导的法则$\frac{\partial \mathbf{a}^{T} \mathbf{X} \mathbf{a}}{\partial \mathbf{a}}&#x3D;2\mathbf{X} \mathbf{a}$可得： $$ \begin{aligned} -\frac{1}{2} \frac{\partial\left(\boldsymbol{x}<em>{j}-\boldsymbol{\mu}</em>{i}\right)^{\top} \boldsymbol{\Sigma}<em>{i}^{-1}\left(\boldsymbol{x}</em>{j}-\boldsymbol{\mu}<em>{i}\right)}{\partial \boldsymbol{\mu}</em>{i}} &amp;&#x3D;-\frac{1}{2} \cdot 2 \boldsymbol{\Sigma}<em>{i}^{-1}\left(\boldsymbol{\mu}</em>{i}-\boldsymbol{x}<em>{j}\right) \ &amp;&#x3D;\boldsymbol{\Sigma}</em>{i}^{-1}\left(\boldsymbol{x}<em>{j}-\boldsymbol{\mu}</em>{i}\right) \end{aligned} $$</p>
<p>因此有： $$ \frac{\partial L L(D)}{\partial \boldsymbol{\mu}<em>{i}}&#x3D;\sum</em>{j&#x3D;1}^{m} \frac{\alpha_{i}}{\sum_{l&#x3D;1}^{k} \alpha_{l} \cdot p\left(\boldsymbol{x}<em>{j} | \boldsymbol{\mu}</em>{l}, \mathbf{\Sigma}<em>{l}\right)} \cdot p\left(\boldsymbol{x}</em>{j} | \boldsymbol{\mu}<em>{i}, \boldsymbol{\Sigma}</em>{i}\right) \cdot \boldsymbol{\Sigma}<em>{i}^{-1}\left(\boldsymbol{x}</em>{j}-\boldsymbol{\mu}_{i}\right)&#x3D;0 $$</p>
<h2 id="主成分分析"><a href="#主成分分析" class="headerlink" title="主成分分析"></a>主成分分析</h2><p>这部分思想和计算方法看了博客啥的大概算是懂了，但是推导（尤其是10.14式子上面那个）没想通，暂时也没找到很好的推导内容。南瓜书里面推导也有，但反正没吃透，这部分等到后面还是要再回过头来看，最好还是要彻底理解原理咋来的（虽然看了也很快就忘）。</p>
<h2 id="计算学习理论"><a href="#计算学习理论" class="headerlink" title="计算学习理论"></a>计算学习理论</h2><ul>
<li>Hoeffding不等式<ul>
<li>m很大的情况下，则$\frac{1}{m}\sum{x_i}$和$\frac{1}{m}\sum{E(x_i)}$很接近。对应到等式右侧，当m很大时，$exp(-2m\epsilon^2)$很小。这表示了m很大的情况下，$\frac{1}{m}\sum{x_i}$和$\frac{1}{m}\sum{E(x_i)}$之间差距大于$\epsilon$的概率很小。</li>
<li>$\epsilon$很大的情况下，表示$\frac{1}{m}\sum{x_i}$和$\frac{1}{m}\sum{E(x_i)}$之间的差距要大过$\epsilon$的可能性非常小。对应到等式右侧，当$\epsilon$很大时，$exp(-2m\epsilon^2)$很小。</li>
</ul>
</li>
<li>McDiarmid不等式<ul>
<li>sup一式表示当我们替换其中一个变量$x_i$为$x_i’$时，对于f函数的影响最多能为多大。当f改变很小（$c_i$很小），则表示函数很稳定。那么对于下式左侧部分的理解，函数值本身与其期望应该比较接近（震荡比较小，充分接近期望），大于$\epsilon$的概率就很小。对应到不等式右侧计算，在$c_i$很小的时候，所得到的的数值也是较小的。</li>
</ul>
</li>
</ul>
<h2 id="PAC学习"><a href="#PAC学习" class="headerlink" title="PAC学习"></a>PAC学习</h2><p>”可分的“的理解：对于概念（真实映射关系），如果它在假设空间内，那么就叫可分的。</p>
<p>具体例子：如果假设空间是用一维线性回归做二分类划分（形如$y&#x3D;ax+b$），那么当空间中有三个点时，则概念在假设空间内，即为”可分的“。对于下图情况，无法通过一根直线在二维平面上将两类样本区分开，即假设空间中不包含概念，则称之为”不可分的“。</p>
<p><img src="/images/%E8%A5%BF%E7%93%9C%E4%B9%A6%E9%9A%8F%E7%AC%94.assets/image-20200907223928012.png" alt="image-20200907223928012"></p>
<p>关于$H$的理解，$H$越大就有更大的概率能够覆盖任意目标概念（在假设空间里面能够找得到概念的概率也就越大），但是当$H$大的时候，想要从多的可能性中确定其中一种的难度也会相应增大。所以对于$H$而言，并不是越大越好，也需要对其大小有所衡量。</p>
<h2 id="VC维"><a href="#VC维" class="headerlink" title="VC维"></a>VC维</h2><p>现实学习任务所面临的通常是<strong>无限假设空间</strong>，欲对此种情形的可学习性进行研究，需度量假设空间的复杂度，最常见的方法是考虑假设空间的“VC维”。</p>
<p>式12.22小于等于号右半部分可以分为前后两块。后面一块的值，随着m的增大而减小（即当数据量增大时，泛化误差与经验误差之间更为接近）；前面一块的值，随着m的增大而增大，对于该反常变化的解释可以理解为”当假设空间很大的情况下，虽然其表示能力会增强，但是找到相应解的难度也会增大。“（上面这句话出自<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Uk4y197Xm?p=4">视频</a>的猜测）。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://liricky.github.io">Ricky Li</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://liricky.github.io/2023/03/20/%E8%A5%BF%E7%93%9C%E4%B9%A6%E9%9A%8F%E7%AC%94/">http://liricky.github.io/2023/03/20/%E8%A5%BF%E7%93%9C%E4%B9%A6%E9%9A%8F%E7%AC%94/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://liricky.github.io" target="_blank">Ricky Li's Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E7%A0%94%E7%A9%B6%E7%94%9F%E9%BB%91%E5%8E%86%E5%8F%B2/">研究生黑历史</a><a class="post-meta__tags" href="/tags/%E8%A5%BF%E7%93%9C%E4%B9%A6/">西瓜书</a></div><div class="post_share"><div class="social-share" data-image="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/03/20/%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" title="阅读笔记"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">阅读笔记</div></div></a></div><div class="next-post pull-right"><a href="/2023/03/20/LeetCode%E7%9A%84%E8%8F%9C%E9%B8%A1%E5%AE%9E%E5%BD%95/" title="LeetCode的菜鸡实录"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">LeetCode的菜鸡实录</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2023/03/20/%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" title="阅读笔记"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-03-20</div><div class="title">阅读笔记</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Ricky Li</div><div class="author-info__description">Learning is all you need</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">5</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/liricky"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">Java & DL; 持续学习and成长ing</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%A5%BF%E7%93%9C%E4%B9%A6%E9%9A%8F%E7%AC%94"><span class="toc-number">1.</span> <span class="toc-text">西瓜书随笔</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E9%9B%86%E3%80%81%E6%B5%8B%E8%AF%95%E9%9B%86%E3%80%81%E9%AA%8C%E8%AF%81%E9%9B%86"><span class="toc-number">1.1.</span> <span class="toc-text">训练集、测试集、验证集</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9F%A5%E5%87%86%E7%8E%87%E3%80%81%E6%9F%A5%E5%85%A8%E7%8E%87"><span class="toc-number">1.2.</span> <span class="toc-text">查准率、查全率</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#F-1-%E5%BA%A6%E9%87%8F"><span class="toc-number">1.3.</span> <span class="toc-text">$F_1$度量</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#x3D-x3D-F-beta-%E5%BA%A6%E9%87%8F-x3D-x3D"><span class="toc-number">1.4.</span> <span class="toc-text">&#x3D;&#x3D;$F_\beta$度量&#x3D;&#x3D;</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E5%88%86%E7%B1%BB%E6%8C%87%E6%A0%87%E8%A1%A1%E9%87%8F"><span class="toc-number">1.5.</span> <span class="toc-text">多分类指标衡量</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ROC%E6%9B%B2%E7%BA%BF"><span class="toc-number">1.6.</span> <span class="toc-text">ROC曲线</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#AUC%E4%B8%8Erank-loss"><span class="toc-number">1.7.</span> <span class="toc-text">AUC与rank-loss</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#x3D-x3D-%E4%BB%A3%E4%BB%B7%E6%95%8F%E6%84%9F%E9%94%99%E8%AF%AF%E7%8E%87%E4%B8%8E%E4%BB%A3%E4%BB%B7%E6%9B%B2%E7%BA%BF-x3D-x3D"><span class="toc-number">1.8.</span> <span class="toc-text">&#x3D;&#x3D;代价敏感错误率与代价曲线&#x3D;&#x3D;</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E4%B8%AA%E6%B5%8B%E8%AF%95%E9%9B%86%E4%B8%80%E7%A7%8D%E7%AE%97%E6%B3%95%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C"><span class="toc-number">1.9.</span> <span class="toc-text">多个测试集一种算法假设检验</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%8E%A8%E5%AF%BC%EF%BC%88%E6%9D%A5%E8%87%AA%E5%8D%97%E7%93%9C%E4%B9%A6%EF%BC%8C%E8%AF%A6%E8%A7%81github%EF%BC%89"><span class="toc-number">1.10.</span> <span class="toc-text">多元线性回归推导（来自南瓜书，详见github）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#l-p-%E8%8C%83%E6%95%B0"><span class="toc-number">1.11.</span> <span class="toc-text">$l_p$范数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B1%80%E9%83%A8%E6%9C%80%E4%BC%98%E4%B8%8E%E5%85%A8%E5%B1%80%E6%9C%80%E4%BC%98"><span class="toc-number">1.12.</span> <span class="toc-text">局部最优与全局最优</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95%E3%80%81%E6%A8%A1%E6%8B%9F%E9%80%80%E7%81%AB"><span class="toc-number">1.13.</span> <span class="toc-text">遗传算法、模拟退火</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%86%B5%E7%9A%84%E5%BA%A6%E9%87%8F%EF%BC%8C%E4%BF%A1%E6%81%AF%E7%86%B5"><span class="toc-number">1.14.</span> <span class="toc-text">熵的度量，信息熵</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CART%E5%88%86%E7%B1%BB%E6%A0%91%E7%AE%97%E6%B3%95"><span class="toc-number">1.15.</span> <span class="toc-text">CART分类树算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#KKT%E6%9D%A1%E4%BB%B6"><span class="toc-number">1.16.</span> <span class="toc-text">KKT条件</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA"><span class="toc-number">1.17.</span> <span class="toc-text">支持向量机</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%B8%E5%87%BD%E6%95%B0"><span class="toc-number">1.17.1.</span> <span class="toc-text">核函数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%8F%E9%AA%8C%E9%A3%8E%E9%99%A9%E3%80%81%E6%9C%9F%E6%9C%9B%E9%A3%8E%E9%99%A9%E3%80%81%E7%BB%93%E6%9E%84%E9%A3%8E%E9%99%A9"><span class="toc-number">1.18.</span> <span class="toc-text">经验风险、期望风险、结构风险</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AE%9A%E7%90%86"><span class="toc-number">1.19.</span> <span class="toc-text">贝叶斯定理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%B3%95%E5%A4%84%E7%90%86%E8%BF%9E%E7%BB%AD%E5%8F%8A%E7%A6%BB%E6%95%A3%E5%B1%9E%E6%80%A7"><span class="toc-number">1.20.</span> <span class="toc-text">朴素贝叶斯法处理连续及离散属性</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%90%89%E5%B8%83%E6%96%AF%E9%87%87%E6%A0%B7"><span class="toc-number">1.21.</span> <span class="toc-text">吉布斯采样</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E7%94%A8%E5%90%89%E5%B8%83%E6%96%AF%E9%87%87%E6%A0%B7"><span class="toc-number">1.21.1.</span> <span class="toc-text">为什么要用吉布斯采样</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AFsampling"><span class="toc-number">1.21.1.1.</span> <span class="toc-text">什么是sampling?</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%90%89%E5%B8%83%E6%96%AF%E9%87%87%E6%A0%B7%E7%9A%84%E9%80%9A%E4%BF%97%E8%A7%A3%E9%87%8A%EF%BC%9F"><span class="toc-number">1.21.1.2.</span> <span class="toc-text">吉布斯采样的通俗解释？</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91"><span class="toc-number">1.22.</span> <span class="toc-text">贝叶斯网</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#EM%E7%AE%97%E6%B3%95"><span class="toc-number">1.23.</span> <span class="toc-text">EM算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#AdaBoost"><span class="toc-number">1.24.</span> <span class="toc-text">AdaBoost</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Boosting%E4%B8%8EBagging"><span class="toc-number">1.25.</span> <span class="toc-text">Boosting与Bagging</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%81%8F%E5%B7%AE-%E6%96%B9%E5%B7%AE%E7%9A%84%E5%B9%B3%E8%A1%A1"><span class="toc-number">1.26.</span> <span class="toc-text">偏差-方差的平衡</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%81%8F%E5%B7%AE-%E6%96%B9%E5%B7%AE%E5%88%86%E8%A7%A3%E7%9A%84%E5%B9%B3%E8%A1%A1"><span class="toc-number">1.27.</span> <span class="toc-text">偏差-方差分解的平衡</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E8%81%9A%E7%B1%BB"><span class="toc-number">1.28.</span> <span class="toc-text">高斯混合聚类</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90"><span class="toc-number">1.29.</span> <span class="toc-text">主成分分析</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA"><span class="toc-number">1.30.</span> <span class="toc-text">计算学习理论</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#PAC%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.31.</span> <span class="toc-text">PAC学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#VC%E7%BB%B4"><span class="toc-number">1.32.</span> <span class="toc-text">VC维</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/03/20/Golang%E8%A7%81%E4%B9%A0/" title="Golang见习">Golang见习</a><time datetime="2023-03-20T06:23:20.000Z" title="发表于 2023-03-20 14:23:20">2023-03-20</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/03/20/%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" title="阅读笔记">阅读笔记</a><time datetime="2023-03-20T06:12:51.000Z" title="发表于 2023-03-20 14:12:51">2023-03-20</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/03/20/%E8%A5%BF%E7%93%9C%E4%B9%A6%E9%9A%8F%E7%AC%94/" title="西瓜书随笔">西瓜书随笔</a><time datetime="2023-03-20T06:12:42.000Z" title="发表于 2023-03-20 14:12:42">2023-03-20</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/03/20/LeetCode%E7%9A%84%E8%8F%9C%E9%B8%A1%E5%AE%9E%E5%BD%95/" title="LeetCode的菜鸡实录">LeetCode的菜鸡实录</a><time datetime="2023-03-20T05:00:23.000Z" title="发表于 2023-03-20 13:00:23">2023-03-20</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/03/18/hello-world/" title="Hello World">Hello World</a><time datetime="2023-03-18T14:13:40.431Z" title="发表于 2023-03-18 22:13:40">2023-03-18</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By Ricky Li</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><div class="js-pjax"></div></div></body></html>