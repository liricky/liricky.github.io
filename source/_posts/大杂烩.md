---
title: 大杂烩
date: 2023-06-01 22:38:10
tags: 
---

## 上传文件失败——HTTP 413

### 问题描述：

脚本向服务器发送报文时上传失败，HTTP状态码413报错

HTTP 响应状态码 **`413 Content Too Large`** 表示请求主体的大小超过了服务器愿意或有能力处理的限度，服务器可能会关闭连接或返回 [`Retry-After`](https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Headers/Retry-After) 标头字段。

此错误通常出现在使用http请求进行文件上传的时候，因为上传文件容易出现大文件，比如超过5m的。

该问题出现于图像数据以本地脚本上传至服务器的过程中，请求以requests.post实现，请求内容在

```python
requests.post(xxx, files:{file:{xxx}})
```

[requests文档](https://requests.readthedocs.io/en/latest/api/?highlight=post#requests.post)

出现问题的文件大小约为2.2M，独立一个文件（pydicom）。

chat求助：

![image-20230601224914370](../images/大杂烩.assets/image-20230601224914370.png)

![image-20230601224929740](../images/大杂烩.assets/image-20230601224929740.png)

### 解决：

经过检查，问题来源于nginx。通过输出返回（报错413）报文的content，发现与nginx有关。经过检查，发送post请求的端口是打到前端去了...（究级猪鼻> ^ <）将请求的端口改成后端后，请求正常完成。

其实也就是只要在requests里面用的是files参数去传文件，不是特别大的情况下不会出现413问题。

另外查了一下nginx相关的参数，可以在http中设置client_max_body_size这个参数的值用于对转发报文的请求体大小进行限制，其默认值为1M（所以也就是为啥别的文件500多K没事，这个里面有一个2M这样的就出问题了）

**==TODO：请求发送到前端，nginx向后端转发的机制==**

## Windows（10）下使用python的tarfile打包tar.gz失败（数据类型错误/数据内容不完整）

### 问题描述：

之前上传数据的脚本在linux服务器上运行时一直正常，但是在我&另一位用了windows上传之后，多次出现上传失败报错。经过debug确定为本地打包数据时出现问题，在本地及服务器上都无法打开出现问题的压缩包。

之后需要在linux服务器上进行控制变量，尝试上传相同批次的数据是否能够复现报错。

初步猜测是windows打包tar.gz文件的过程中因为格式问题导致出现的偶发报错（是的，并不是100%失败）。自己想的初步解决思路是本地直接就近重试打开以判断是否打包失败，但是还需要确定两个问题，一个是打包的过程中间已经出现问题卡死，还是打包部分正常运行完，但是后续无法再解压打开（目前倾向于后者，所以考虑就近重试）。当然就近重试还有一个问题就是会不会重试依然失败，即需要明确导致失败的原因和场景。

### 解决：

```python
# TODO
```

猜测是打包的过程中文件头出现了问题。查找后发现，文件头有多种类型，如下：

![image-20230605204550460](../images/大杂烩.assets/image-20230605204550460.png)

不过在修改tarfile.open的format读取参数之后，似乎还是会有出现压缩包数据错误的现象。（从文件尺寸上来看，似乎压缩包中的内容也不完整）

目前采用的解决方案是原地进行一次解压获取，就近重试并捕获异常，从而确定数据被正常压缩并且可以打开。对打开的tarfile文件使用getmembers方法可以获取其中的内容信息（用于判断得到的压缩包是否正常）。

概率性问题，先分步拆解问题，定位具体出错在哪；然后考虑成功和出错的差异是什么。当然这里面还是需要一些经验的，文件头出错其实之前有碰到过，还是要积累总结。

## kubernetes集群环境下，pytorch训练卡顿的可能原因排查

### 问题描述：

深度学习在集群环境下进行（分布式）训练时，训练任务出现卡顿（1. 训练正常进行但是速度较平常慢；2. 训练中的某些轮次长时间未继续执行下去，但在操作或等待很长一段时间后重新回复，在此期间对该任务所在节点的其它任务产生了影响；3. 训练中的某些轮次完全卡住无法继续执行下去，并且对同一节点上的其它任务产生了影响）

### 排查：

这个因为牵扯到太多因素了，所以不指望能彻底解决，可以降低发生的频率感觉就很好了。

#### 可能的原因1：

pytorch的dataloader中设置了过大的num_workers。由于dataloder在取数据（和做预处理）的过程中是异步的，大致的思想是有num_workers+1个worker分别取数据并处理，然后弄完丢到一个池子里面。等到迭代的时候（get_item）再从池子里面取出来。那么因为worker之间并不能够主动去分辨哪个任务更为紧急，所以就会出现某一个batch的数据马上就要用了，但是还没在池子里，那么就会出问题。这个可能和起了多个进程（num_worker起一个进程）然后产生的上下文切换啥的有关系？具体还需要等到之后的实验跑了才能确定，不过在num_worker大于申请核心数的情况下，确实会出现上面所描述的情况。这种情况直接导致的应该是卡顿的第一种情况，或者介于第一种情况和第二种情况之间。不过在硬性限制能够使用的cpu核心数之后，这个问题应该不会再影响到同一节点上的其它用户（待确认）。

这个问题经过实验之后感觉非常玄学，即pytorch的num_workers的设置较大（如大于等于16）时，进行训练就可能产生资源的竞争，进而引发一系列问题。所以一种比较简单的解决方法就是不开num_workers或者设置的比较小，不过这样确实会对训练的速度（主要是数据加载上的瓶颈）产生影响。另外就是设置的num_workers比实际核心数大一阶2的倍数，似乎影响的情况并不明显？（这个最好之后再能跑实验进行一个验证，目前采用的是cpu的使用率限制，应该还可以用cgroup进行更为细致的核心限制；[cgroup参考资料1](https://zhuanlan.zhihu.com/p/81668069)，[cgroup参考资料2](https://zhuanlan.zhihu.com/p/433327341)）

#### 可能的问题2：

在使用中观察到，卡顿过程中，物理机的某些CPU核心存在长时间100%占用无法释放的情况。这个现象可能是由k8s的调度策略所导致的，但是暂时还未经过实验验证；另一种猜想是由系统内核造成的。总之短期内并不太好解决这个问题，所以暂时采用终止并重启的方式去打破阻塞状态。

#### 可能的问题3：

速度慢可能和IO的读取速度存在一定的关联。之前的策略是从nfs上进行数据的读取，这中间存在着网络传输的时间开销。为了能够减少这部分的时间损耗，可以使用类似redis的形式在nfs上套一个缓存，即**Alluxio**（在 **文件系统-存储** 中叙述）。通过这个方式能够让数据的读取从远程nfs变为优先命中缓存。Alluxio先存放的本地的内存空间中，随后是本地磁盘，然后是集群不同节点的内存\磁盘，最后都不命中才会到远程nfs上读取。从目前的实验效果上来说，第一个Epoch之后的IO时间是有降低的（测试约10%-20%），不过这个暂时还没有验证数据淘汰可能引发的问题。技术手段上来说可以做一个了解。

#### ==可能的问题4==:

TLB shutdown

这个其实算看到的现象。

TLB与快表

```java
// TODO
```

## 正确的上线流程

1. 拉取自己的dev分支
2. 修改代码，并自测
3. 提交PR，合并至dev
4. dev合并至test
5. test分支跑jenkins出镜像
6. 镜像在测试环境测试
7. test合并至master
8. master分支跑jenkins出镜像
9. 生产环境上线

## 工具插件、脚本血泪史

不管是啥插件、脚本，就算是再临时性的东西都要留log！！！然后留的log一定要格式标准化一点，方便日后要统计或者分析的时候方便去扒（正则或者别的方式）。**==其实比较好的方式就是用logging==**

还有一个需要特别注意的就是，webhook或者别的报警的逻辑和日志的逻辑能拆分尽量拆分。不然报警里面到时候一堆需求改这改那，回头整理日志分析的时候一个是判断条件可能不统一，还有就是逻辑还得逆向处理。真得是把自己弄的想死。目前的感悟是，报警这种东西就专门归报警干，别和日志搅和在一起。报警只负责即时抓到问题，鬼才会管后面恢复记录啥的。而日志是要留完整信息的，后续会再捞出来分析统计。因此，就算像webhook这种报警能够在那个节点抓到日志，并且返回而且提示，这玩意也不适合直接当日志去使用！！！

***==可扩展性==***

## 语法糖

### 装饰器

本质是返回一个函数

### 闭包

```python
# TODO
```

## K8S强制删除POD

[参考资料](https://blog.csdn.net/ArdenL/article/details/129180078)

```bash
kubectl delete pod <podName> -n <nsName> --force --grace-period=0
```

## 进程的状态

### 进程状态为 D (Uninterruptible Sleep)

#### 什么是 D 进程？

[相关资料1](https://zhuanlan.zhihu.com/p/509807556)、[相关资料2](https://www.cnblogs.com/embedded-linux/p/7043569.html#:~:text=断开连接再登录，执行%20ps%20axf%20则看到刚才的%20df%20进程状态位已变成了%20D,，kill%20-9%20无法杀灭%E3%80%82%20正确的处理方式，是马上恢复%20NFS%20服务端，再度提供服务，刚才挂起的%20df%20进程发现了其苦苦等待的资源，便完成任务，自动消亡%E3%80%82)

#### 状态为 D 的进程如何处理

kill -9无法处理；考虑提供资源解除状态，不然的话就得重启了（貌似有那种修改线程状态的办法，但是比较危险？所以不用）

## mysql多表联查优化

主要是发现一个复杂的4-5表联查的接口超时（10s+）。有筛选条件的时候正常，搜索全部的时候超时。

这个最后看了逻辑，实际上有多做了一次select *的全表子查询，这个查询在有其它筛选条件的情况下是起作用的，但是在没有筛选条件时相当于白做了一次select *的全表查询，所以导致了超时。增加条件筛选之后回复正常。

[EXPLAIN的使用](https://blog.csdn.net/fsdfkjai/article/details/121770629)

这里正好顺便补充一下关于数据库多表联查优化相关的一些内容。

[**==参考资料==**](https://blog.csdn.net/qq_41708993/article/details/126235768)

## 虚拟机与容器

[参考资料](https://zhuanlan.zhihu.com/p/339709750)

- 安全性
- 隔离性

容器不依赖于操作系统，通过Linux的Namespace和Cgroups技术对应用程度进程进行隔离和限制。Namespace的作用是隔离，它让应用程序只能看到该Namespace内的世界；而Cgroups的作用是限制分配给进程的宿主机资源。对于宿主机而言，这些被“隔离”的进程与其他进程并没有太大的区别。

容器与rootfs，可参考**存储**中的rootfs相关内容

## VFIO

[参考资料](https://zhuanlan.zhihu.com/p/534574765)

关于上面资料的一个疑问：单机训练数据的流向与集群训练数据的流向，什么时候经过网卡，什么时候不经过网卡，如何优化？

参考资料里面的这两句话感觉怪怪的：

```
图中红线的部门，是两个 Context Entry 指向了同一个页表。这种情况在虚拟化场景中的典型用法就是这两个Context Entry 对应的不同 PCIe 设备属于同一个虚机，那样 IOMMU 在将 GPA->HPA 过程中要遵循同一规则
由图中可知，每个具有 Source Identifier(包含Bus、Device、Func)的设备都会具有一个 Context Entry。如果不这样做，所有设备共用同一个页表，隶属于不同虚机的不同 GPA 就会翻译成相同 HPA，会产生问题
```

我理解的是，指向同一个页表不一定是相同的内存地址，只不过是同一个虚机用同一个页表映射？虚机之间就有隔离性？

![image-20230814001138692](../images/大杂烩.assets/image-20230814001138692.png)

```java
// TODO
```

## /dev/null与/dev/zero

[相关资料1](https://www.cnblogs.com/pipci/p/11412082.html#:~:text=%2Fdev%2Fnull%20它是空设备，也称为位桶（bit%20bucket）或者黑洞%20%28black%20hole%29%E3%80%82%20你可以向它输入任何数据，但任何写入它的数据都会被抛弃%E3%80%82,通常用于处理不需要的输出流%E3%80%82%20（当然，它也可以作为空的输入流）%20%2Fdev%2Fzero%20该设备无穷尽地提供空字符（ASCII%20NUL%2C%200x00），可以使用任何你需要的数目%E3%80%82%20它通常用于向设备或文件写入字符串0，用于初始化数据存储%E3%80%82)

[相关资料2](https://zhuanlan.zhihu.com/p/50879912)

在类Unix操作系统中，设备节点并不一定要对应物理设备。没有这种对应关系的设备被称之为伪设备。操作系统运用了它们实现多种多样的功能，/dev/null和/dev/zero就是这样的设备，类似的还有/dev/urandom、/dev/tty等。

### /dev/null

在类Unix系统中，/dev/null（空设备文件或黑洞文件）是一个特殊的设备文件，所有写入其中的数据，都会被丢弃的无影无踪，/dev/null通常被用于丢弃不需要的数据输出，或作为用于输入流的空文件。这些操作通常由重定向完成。

```bash
cat /dev/null > /etc/hosts  # 将读取黑洞设备写入到/etc/hosts，表示清空hosts文件
```

### /dev/zero

和/dev/null类似，/dev/zero也是一个特殊的字符设备文件，当我们使用或读取它的时候，它会提供无限连续不断的空的数据流（特殊的数据格式流）。

/dev/zero文件的常见应用场景有二：

1. /dev/zero文件覆盖其他文件信息。

2. 产生指定大小的空文件，例如：交换文件、模拟虚拟文件系统等。

### /dev/urandom

`/dev/random` 和 `/dev/urandom` 是 Linux 上的字符设备文件，它们是随机数生成器，为系统提供随机数

[/dev/random 和 /dev/urandom 的原理](https://zhuanlan.zhihu.com/p/427489847)

### /dev/tty

[Linux系统：/dev/tty、/dev/tty0 和 /dev/console之间的区别](https://zhuanlan.zhihu.com/p/632099551)

`/dev/tty`文件是一个特殊的文件，代表当前进程的终端，它显示与当前 SSH 会话关联的终端。

## 训练服务器数据的流向（网卡）

```python
# TODO
```

## nvidia-container-runtime环境变量

NVIDIA_VISIBLE_DEVICES=all，该环境变量控制以nvidia-container-runtime为runtime的docker，在运行时将哪些显卡设备传入容器中。对于使用runc为runtime的docker，在运行时如果想要将显卡传入容器中，直接使用--gpus all参数即可。

## 2>&1

[如何理解Linux shell中的“2>&1”](https://zhuanlan.zhihu.com/p/47765176)

每个程序在运行后，都会至少打开三个文件描述符，分别是0：标准输入；1：标准输出；2：标准错误。

2>&1表明将文件描述2（标准错误输出）的内容重定向到文件描述符1（标准输出），为什么1前面需要&？当没有&时，1会被认为是一个普通的文件，有&表示重定向的目标不是一个文件，而是一个文件描述符

在调用脚本时，可使用2>&1来将标准错误输出重定向

### command > file 2>&1 
首先是command > file将标准输出重定向到file中， 2>&1 是标准错误拷贝了标准输出的行为，也就是同样被重定向到file中，最终结果就是标准输出和错误都被重定向到file中。

### command 2>&1 >file 
2>&1 标准错误拷贝了标准输出的行为（先拷贝的行为，但是此时标准输出的行为还没有重定向，所以此时标准错误的行为也是输出到终端。之后，标准输出行为才定向到了file。），但此时标准输出还是在终端。>file 后输出才被重定向到file，但标准错误仍然保持在终端。

## dd命令

dd命令，主要功能为转换和复制文件，用指定大小的块拷贝一个文件，并在拷贝的同时进行指定的转换。

[dd命令详解，语法，参数，dd示例](https://blog.csdn.net/wangzhicheng987/article/details/121923220#:~:text=dd命令%2C主要功能为转换和复制文件%E3%80%82%20在Linux中，硬件的设备驱动和特殊设备文件,也是文件；dd也可以直接读取或写入到这些文件%E3%80%82%20dd：用指定大小的块拷贝一个文件，并在拷贝的同时进行指定的转换%E3%80%82)

[linux下的dd命令使用详解](https://zhuanlan.zhihu.com/p/78414544)

## DDD（领域驱动设计，Domain-driven design）

[相关资料](https://zhuanlan.zhihu.com/p/109114670)

《领域驱动设计模式、原理与实践》

## 涉及数据库的业务实现原则

能不用联表查询就尽量不用，可以使用单表查询并在内存中拼接替代联表查询（内存操作的速度一定快于请求连接访问数据库）；保留必要的冗余，可以在业务中避免联表查询的发生。

## 对称加密与非对称加密

[相关资料](https://zhuanlan.zhihu.com/p/83644573)

对称和不对称指的就是**加密和解密用的秘钥是不是同一个**。

非对称加密用的是一对秘钥，分别叫做公钥（public key）和私钥（private key），也叫非对称秘钥。非对称秘钥既可以用于加密还可以用于认证

## 同一张表查询

同一张表的查询合并为一次，可以取出多个字段的值之后再在内存中处理

## GROUP_CONCAT

MySQL中group_concat函数
完整的语法如下：

group_concat([DISTINCT] 要连接的字段 [Order BY ASC/DESC 排序字段] [Separator '分隔符'])

[相关资料](https://blog.csdn.net/ys410900345/article/details/44828571)

**奇特的用法**：查询结果中的多个字段都需要做distinct就可以使用group_concat包裹一层之后返回查询结果。查询结果将会表现为一个字段，这个字段中使用逗号分隔所有的distinct结果值。

## rbac权限控制

[相关资料](https://zhuanlan.zhihu.com/p/158752542#:~:text=RBAC模型（Role-Based%20Access%20Control：基于角色的访问控制）模型是比较早期提出的权限实现模型，在多用户计算机时期该思想即被提出，其中以美国George%20Mason大学信息安全技术实验室（LIST）提出的,RBAC96%20模型最具有代表，并得到了普遍的公认%E3%80%82%20RBAC认为权限授权的过程可以抽象地概括为：Who是否可以对What进行How的访问操作，并对这个逻辑表达式进行判断是否为True的求解过程，也即是将权限问题转换为Who、What、How的问题，Who、What、How构成了访问权限三元组，具体的理论可以参考%20RBAC96%20%E3%80%82)

对于角色概念的理解：角色实际上是在描述多个权限的集合，借助角色这个桥梁，可以实现对用户持有权限的批量修改

### rbac with domain

见Casbin

## UML时序图规范

[时序图格式规范](https://zhuanlan.zhihu.com/p/342655946)

发送请求为实线，返回请求为虚线，同步消息为实心箭头，异步消息为方向箭头

[UML图 | 时序图（顺序、序列）绘制](https://zhuanlan.zhihu.com/p/422509874)

[UML时序图详解](https://zhuanlan.zhihu.com/p/629630251)

## Casbin

rbac的一种开源实现方式

[官方文档](https://casbin.org/docs/overview)

## 任务调度

airflow，phoenix，quartz
