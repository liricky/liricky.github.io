---
title: 大杂烩
date: 2023-06-01 22:38:10
tags: 
---

## 上传文件失败——HTTP 413

### 问题描述：

脚本向服务器发送报文时上传失败，HTTP状态码413报错

HTTP 响应状态码 **`413 Content Too Large`** 表示请求主体的大小超过了服务器愿意或有能力处理的限度，服务器可能会关闭连接或返回 [`Retry-After`](https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Headers/Retry-After) 标头字段。

此错误通常出现在使用http请求进行文件上传的时候，因为上传文件容易出现大文件，比如超过5m的。

该问题出现于图像数据以本地脚本上传至服务器的过程中，请求以requests.post实现，请求内容在

```python
requests.post(xxx, files:{file:{xxx}})
```

[requests文档](https://requests.readthedocs.io/en/latest/api/?highlight=post#requests.post)

出现问题的文件大小约为2.2M，独立一个文件（pydicom）。

chat求助：

![image-20230601224914370](/images/大杂烩.assets/image-20230601224914370.png)

![image-20230601224929740](/images/大杂烩.assets/image-20230601224929740.png)

### 解决：

经过检查，问题来源于nginx。通过输出返回（报错413）报文的content，发现与nginx有关。经过检查，发送post请求的端口是打到前端去了...（究级猪鼻> ^ <）将请求的端口改成后端后，请求正常完成。

其实也就是只要在requests里面用的是files参数去传文件，不是特别大的情况下不会出现413问题。

另外查了一下nginx相关的参数，可以在http中设置client_max_body_size这个参数的值用于对转发报文的请求体大小进行限制，其默认值为1M（所以也就是为啥别的文件500多K没事，这个里面有一个2M这样的就出问题了）

**==TODO：请求发送到前端，nginx向后端转发的机制==**

## Windows（10）下使用python的tarfile打包tar.gz失败（数据类型错误/数据内容不完整）

### 问题描述：

之前上传数据的脚本在linux服务器上运行时一直正常，但是在我&另一位用了windows上传之后，多次出现上传失败报错。经过debug确定为本地打包数据时出现问题，在本地及服务器上都无法打开出现问题的压缩包。

之后需要在linux服务器上进行控制变量，尝试上传相同批次的数据是否能够复现报错。

初步猜测是windows打包tar.gz文件的过程中因为格式问题导致出现的偶发报错（是的，并不是100%失败）。自己想的初步解决思路是本地直接就近重试打开以判断是否打包失败，但是还需要确定两个问题，一个是打包的过程中间已经出现问题卡死，还是打包部分正常运行完，但是后续无法再解压打开（目前倾向于后者，所以考虑就近重试）。当然就近重试还有一个问题就是会不会重试依然失败，即需要明确导致失败的原因和场景。

### 解决：

```python
# TODO
```

猜测是打包的过程中文件头出现了问题。查找后发现，文件头有多种类型，如下：

![image-20230605204550460](/images/大杂烩.assets/image-20230605204550460.png)

不过在修改tarfile.open的format读取参数之后，似乎还是会有出现压缩包数据错误的现象。（从文件尺寸上来看，似乎压缩包中的内容也不完整）

目前采用的解决方案是原地进行一次解压获取，就近重试并捕获异常，从而确定数据被正常压缩并且可以打开。对打开的tarfile文件使用getmembers方法可以获取其中的内容信息（用于判断得到的压缩包是否正常）。

概率性问题，先分步拆解问题，定位具体出错在哪；然后考虑成功和出错的差异是什么。当然这里面还是需要一些经验的，文件头出错其实之前有碰到过，还是要积累总结。

## kubernetes集群环境下，pytorch训练卡顿的可能原因排查

### 问题描述：

深度学习在集群环境下进行（分布式）训练时，训练任务出现卡顿（1. 训练正常进行但是速度较平常慢；2. 训练中的某些轮次长时间未继续执行下去，但在操作或等待很长一段时间后重新回复，在此期间对该任务所在节点的其它任务产生了影响；3. 训练中的某些轮次完全卡住无法继续执行下去，并且对同一节点上的其它任务产生了影响）

### 排查：

这个因为牵扯到太多因素了，所以不指望能彻底解决，可以降低发生的频率感觉就很好了。

#### 可能的原因1：

pytorch的dataloader中设置了过大的num_workers。由于dataloder在取数据（和做预处理）的过程中是异步的，大致的思想是有num_workers+1个worker分别取数据并处理，然后弄完丢到一个池子里面。等到迭代的时候（get_item）再从池子里面取出来。那么因为worker之间并不能够主动去分辨哪个任务更为紧急，所以就会出现某一个batch的数据马上就要用了，但是还没在池子里，那么就会出问题。这个可能和起了多个进程（num_worker起一个进程）然后产生的上下文切换啥的有关系？具体还需要等到之后的实验跑了才能确定，不过在num_worker大于申请核心数的情况下，确实会出现上面所描述的情况。这种情况直接导致的应该是卡顿的第一种情况，或者介于第一种情况和第二种情况之间。不过在硬性限制能够使用的cpu核心数之后，这个问题应该不会再影响到同一节点上的其它用户（待确认）。

这个问题经过实验之后感觉非常玄学，即pytorch的num_workers的设置较大（如大于等于16）时，进行训练就可能产生资源的竞争，进而引发一系列问题。所以一种比较简单的解决方法就是不开num_workers或者设置的比较小，不过这样确实会对训练的速度（主要是数据加载上的瓶颈）产生影响。另外就是设置的num_workers比实际核心数大一阶2的倍数，似乎影响的情况并不明显？（这个最好之后再能跑实验进行一个验证，目前采用的是cpu的使用率限制，应该还可以用cgroup进行更为细致的核心限制；[cgroup参考资料1](https://zhuanlan.zhihu.com/p/81668069)，[cgroup参考资料2](https://zhuanlan.zhihu.com/p/433327341)）

#### 可能的问题2：

在使用中观察到，卡顿过程中，物理机的某些CPU核心存在长时间100%占用无法释放的情况。这个现象可能是由k8s的调度策略所导致的，但是暂时还未经过实验验证；另一种猜想是由系统内核造成的。总之短期内并不太好解决这个问题，所以暂时采用终止并重启的方式去打破阻塞状态。

#### 可能的问题3：

速度慢可能和IO的读取速度存在一定的关联。之前的策略是从nfs上进行数据的读取，这中间存在着网络传输的时间开销。为了能够减少这部分的时间损耗，可以使用类似redis的形式在nfs上套一个缓存，即**Alluxio**（在 **文件系统-存储** 中叙述）。通过这个方式能够让数据的读取从远程nfs变为优先命中缓存。Alluxio先存放的本地的内存空间中，随后是本地磁盘，然后是集群不同节点的内存\磁盘，最后都不命中才会到远程nfs上读取。从目前的实验效果上来说，第一个Epoch之后的IO时间是有降低的（测试约10%-20%），不过这个暂时还没有验证数据淘汰可能引发的问题。技术手段上来说可以做一个了解。

#### ==可能的问题4==:

TLB shutdown

这个其实算看到的现象。

TLB与快表

```java
// TODO
```

## 正确的上线流程

1. 拉取自己的dev分支
2. 修改代码，并自测
3. 提交PR，合并至dev
4. dev合并至test
5. test分支跑jenkins出镜像
6. 镜像在测试环境测试
7. test合并至master
8. master分支跑jenkins出镜像
9. 生产环境上线

## 工具插件、脚本血泪史

不管是啥插件、脚本，就算是再临时性的东西都要留log！！！然后留的log一定要格式标准化一点，方便日后要统计或者分析的时候方便去扒（正则或者别的方式）。**==其实比较好的方式就是用logging==**

还有一个需要特别注意的就是，webhook或者别的报警的逻辑和日志的逻辑能拆分尽量拆分。不然报警里面到时候一堆需求改这改那，回头整理日志分析的时候一个是判断条件可能不统一，还有就是逻辑还得逆向处理。真得是把自己弄的想死。目前的感悟是，报警这种东西就专门归报警干，别和日志搅和在一起。报警只负责即时抓到问题，鬼才会管后面恢复记录啥的。而日志是要留完整信息的，后续会再捞出来分析统计。因此，就算像webhook这种报警能够在那个节点抓到日志，并且返回而且提示，这玩意也不适合直接当日志去使用！！！

***==可扩展性==***

## 语法糖

### 装饰器

本质是返回一个函数

### 闭包

```python
# TODO
```

## K8S强制删除POD

[参考资料](https://blog.csdn.net/ArdenL/article/details/129180078)

```bash
kubectl delete pod <podName> -n <nsName> --force --grace-period=0
```

## 进程的状态

### 进程状态为 D (Uninterruptible Sleep)

#### 什么是 D 进程？

[相关资料1](https://zhuanlan.zhihu.com/p/509807556)、[相关资料2](https://www.cnblogs.com/embedded-linux/p/7043569.html#:~:text=断开连接再登录，执行%20ps%20axf%20则看到刚才的%20df%20进程状态位已变成了%20D,，kill%20-9%20无法杀灭%E3%80%82%20正确的处理方式，是马上恢复%20NFS%20服务端，再度提供服务，刚才挂起的%20df%20进程发现了其苦苦等待的资源，便完成任务，自动消亡%E3%80%82)

#### 状态为 D 的进程如何处理

kill -9无法处理；考虑提供资源解除状态，不然的话就得重启了（貌似有那种修改线程状态的办法，但是比较危险？所以不用）

## mysql多表联查优化

主要是发现一个复杂的4-5表联查的接口超时（10s+）。有筛选条件的时候正常，搜索全部的时候超时。

这个最后看了逻辑，实际上有多做了一次select *的全表子查询，这个查询在有其它筛选条件的情况下是起作用的，但是在没有筛选条件时相当于白做了一次select *的全表查询，所以导致了超时。增加条件筛选之后回复正常。

[EXPLAIN的使用](https://blog.csdn.net/fsdfkjai/article/details/121770629)

这里正好顺便补充一下关于数据库多表联查优化相关的一些内容。

[**==参考资料==**](https://blog.csdn.net/qq_41708993/article/details/126235768)

## 虚拟机与容器

[参考资料](https://zhuanlan.zhihu.com/p/339709750)

- 安全性
- 隔离性

容器不依赖于操作系统，通过Linux的Namespace和Cgroups技术对应用程度进程进行隔离和限制。Namespace的作用是隔离，它让应用程序只能看到该Namespace内的世界；而Cgroups的作用是限制分配给进程的宿主机资源。对于宿主机而言，这些被“隔离”的进程与其他进程并没有太大的区别。

容器与rootfs，可参考**存储**中的rootfs相关内容

## VFIO

[参考资料](https://zhuanlan.zhihu.com/p/534574765)

关于上面资料的一个疑问：单机训练数据的流向与集群训练数据的流向，什么时候经过网卡，什么时候不经过网卡，如何优化？

```java
// TODO
```

