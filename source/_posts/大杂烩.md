---
title: 大杂烩
date: 2023-06-01 22:38:10
tags: 
---

## 上传文件失败——HTTP 413

### 问题描述：

脚本向服务器发送报文时上传失败，HTTP状态码413报错

HTTP 响应状态码 **`413 Content Too Large`** 表示请求主体的大小超过了服务器愿意或有能力处理的限度，服务器可能会关闭连接或返回 [`Retry-After`](https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Headers/Retry-After) 标头字段。

此错误通常出现在使用http请求进行文件上传的时候，因为上传文件容易出现大文件，比如超过5m的。

该问题出现于图像数据以本地脚本上传至服务器的过程中，请求以requests.post实现，请求内容在

```python
requests.post(xxx, files:{file:{xxx}})
```

[requests文档](https://requests.readthedocs.io/en/latest/api/?highlight=post#requests.post)

出现问题的文件大小约为2.2M，独立一个文件（pydicom）。

chat求助：

![image-20230601224914370](/images/大杂烩.assets/image-20230601224914370.png)

![image-20230601224929740](/images/大杂烩.assets/image-20230601224929740.png)

### 解决：

经过检查，问题来源于nginx。通过输出返回（报错413）报文的content，发现与nginx有关。经过检查，发送post请求的端口是打到前端去了...（究级猪鼻> ^ <）将请求的端口改成后端后，请求正常完成。

其实也就是只要在requests里面用的是files参数去传文件，不是特别大的情况下不会出现413问题。

另外查了一下nginx相关的参数，可以在http中设置client_max_body_size这个参数的值用于对转发报文的请求体大小进行限制，其默认值为1M（所以也就是为啥别的文件500多K没事，这个里面有一个2M这样的就出问题了）

**==TODO：请求发送到前端，nginx向后端转发的机制==**

## Windows（10）下使用python的tarfile打包tar.gz失败（数据类型错误/数据内容不完整）

### 问题描述：

之前上传数据的脚本在linux服务器上运行时一直正常，但是在我&另一位用了windows上传之后，多次出现上传失败报错。经过debug确定为本地打包数据时出现问题，在本地及服务器上都无法打开出现问题的压缩包。

之后需要在linux服务器上进行控制变量，尝试上传相同批次的数据是否能够复现报错。

初步猜测是windows打包tar.gz文件的过程中因为格式问题导致出现的偶发报错（是的，并不是100%失败）。自己想的初步解决思路是本地直接就近重试打开以判断是否打包失败，但是还需要确定两个问题，一个是打包的过程中间已经出现问题卡死，还是打包部分正常运行完，但是后续无法再解压打开（目前倾向于后者，所以考虑就近重试）。当然就近重试还有一个问题就是会不会重试依然失败，即需要明确导致失败的原因和场景。

### 解决：

```python
# TODO
```

猜测是打包的过程中文件头出现了问题。查找后发现，文件头有多种类型，如下：

![image-20230605204550460](/images/大杂烩.assets/image-20230605204550460.png)

不过在修改tarfile.open的format读取参数之后，似乎还是会有出现压缩包数据错误的现象。（从文件尺寸上来看，似乎压缩包中的内容也不完整）

目前采用的解决方案是原地进行一次解压获取，就近重试并捕获异常，从而确定数据被正常压缩并且可以打开。对打开的tarfile文件使用getmembers方法可以获取其中的内容信息（用于判断得到的压缩包是否正常）。

概率性问题，先分步拆解问题，定位具体出错在哪；然后考虑成功和出错的差异是什么。当然这里面还是需要一些经验的，文件头出错其实之前有碰到过，还是要积累总结。

## kubernetes集群环境下，pytorch训练卡顿的可能原因排查

### 问题描述：

深度学习在集群环境下进行（分布式）训练时，训练任务出现卡顿（1. 训练正常进行但是速度较平常慢；2. 训练中的某些轮次长时间未继续执行下去，但在操作或等待很长一段时间后重新回复，在此期间对该任务所在节点的其它任务产生了影响；3. 训练中的某些轮次完全卡住无法继续执行下去，并且对同一节点上的其它任务产生了影响）

### 排查：

这个因为牵扯到太多因素了，所以不指望能彻底解决，可以降低发生的频率感觉就很好了。

#### 可能的原因1：

pytorch的dataloader中设置了过大的num_workers。由于dataloder在取数据（和做预处理）的过程中是异步的，大致的思想是有num_workers+1个worker分别取数据并处理，然后弄完丢到一个池子里面。等到迭代的时候（get_item）再从池子里面取出来。那么因为worker之间并不能够主动去分辨哪个任务更为紧急，所以就会出现某一个batch的数据马上就要用了，但是还没在池子里，那么就会出问题。这个可能和起了多个进程（num_worker起一个进程）然后产生的上下文切换啥的有关系？具体还需要等到之后的实验跑了才能确定，不过在num_worker大于申请核心数的情况下，确实会出现上面所描述的情况。这种情况直接导致的应该是卡顿的第一种情况，或者介于第一种情况和第二种情况之间。不过在硬性限制能够使用的cpu核心数之后，这个问题应该不会再影响到同一节点上的其它用户（待确认）。

#### 可能的问题2：

在使用中观察到，卡顿过程中，物理机的某些CPU核心存在长时间100%占用无法释放的情况。这个现象可能是由k8s的调度策略所导致的，但是暂时还未经过实验验证；另一种猜想是由系统内核造成的。总之短期内并不太好解决这个问题，所以暂时采用终止并重启的方式去打破阻塞状态。

#### 可能的问题3：

速度慢可能和IO的读取速度存在一定的关联。之前的策略是从nfs上进行数据的读取，这中间存在着网络传输的时间开销。为了能够减少这部分的时间损耗，可以使用类似redis的形式在nfs上套一个缓存，即**Alluxio**（在 **文件系统-存储** 中叙述）。通过这个方式能够让数据的读取从远程nfs变为优先命中缓存。Alluxio先存放的本地的内存空间中，随后是本地磁盘，然后是集群不同节点的内存\磁盘，最后都不命中才会到远程nfs上读取。从目前的实验效果上来说，第一个Epoch之后的IO时间是有降低的（测试约10%-20%），不过这个暂时还没有验证数据淘汰可能引发的问题。技术手段上来说可以做一个了解。
